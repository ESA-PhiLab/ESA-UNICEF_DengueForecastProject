{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7audVqxFBDWh",
   "metadata": {
    "id": "7audVqxFBDWh"
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UdxUs05bahYS",
   "metadata": {
    "id": "UdxUs05bahYS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# insert your desired path to work on\n",
    "import os\n",
    "from os.path import join\n",
    "project_path = os.path.dirname(os.getcwd())\n",
    "os.chdir(join('..','data'))\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8tZCHvverUb_",
   "metadata": {
    "id": "8tZCHvverUb_"
   },
   "source": [
    "Run the following section one time per session. This cell links the code folder to the python exectution path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hftunTO1rHKZ",
   "metadata": {
    "id": "hftunTO1rHKZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(join(project_path, 'code'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33R-Be_BrUuK",
   "metadata": {
    "id": "33R-Be_BrUuK"
   },
   "source": [
    "The following cell allows Jupyter Notebooks to detect changes in external code and to automatically update it without restarting the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d77b30-7bf9-4302-9def-c4f4ea4a5963",
   "metadata": {
    "id": "12d77b30-7bf9-4302-9def-c4f4ea4a5963",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K2C_kf2otfQF",
   "metadata": {
    "id": "K2C_kf2otfQF"
   },
   "source": [
    "Plots settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LENlVPDfteud",
   "metadata": {
    "id": "LENlVPDfteud",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'family':'Arial', 'size':'15', 'weight':'normal'}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f16e241-e507-4edd-a900-5594f835b4e6",
   "metadata": {},
   "source": [
    "Set folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8d95b-6ce2-4ce0-a419-4628ceea6e2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'main_brazil': 'Brazil',\n",
    "    'main_peru': 'Peru',\n",
    "    'baseline': join(project_path, \"baseline_models\"),\n",
    "    'output': join(project_path, \"code\", \"saved_models\"),\n",
    "    'metrics': join(project_path, \"code\", \"metrics\")\n",
    "}\n",
    "\n",
    "# List comprehension for the folder structure code\n",
    "[os.makedirs(val, exist_ok=True) for key, val in config.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d1eed-54e9-4f76-b8f8-42587caaabaa",
   "metadata": {
    "id": "8d3d1eed-54e9-4f76-b8f8-42587caaabaa",
    "tags": []
   },
   "source": [
    "# **AI4Dengue forecasting**\n",
    "![](https://drive.google.com/uc?export=view&id=1J5Bt5Cks-e2IV-dEJLHJkuwXFJNFAZgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e0945-3bd3-4660-87bc-ebc9438535fc",
   "metadata": {
    "id": "272e0945-3bd3-4660-87bc-ebc9438535fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from config import DEP_NAMES, GROUPED_VARS, DATA_REDUCER_SETTINGS, DATA_PROCESSING_SETTINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Mmz-HTHisxH3",
   "metadata": {
    "id": "Mmz-HTHisxH3",
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e2eae-97ff-475f-bfd5-78e8dcee981d",
   "metadata": {
    "id": "717e2eae-97ff-475f-bfd5-78e8dcee981d"
   },
   "source": [
    "## Load the dataframe\n",
    "**This dataframe comprises all the variables (climatic, epidemiological etc.) acquired for each Department during a defined number of years.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5208ca-5c08-4b12-b4bf-37a49f338d25",
   "metadata": {
    "id": "8e5208ca-5c08-4b12-b4bf-37a49f338d25"
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(join(config['main_brazil'], \"Brazil_UF_dengue_monthly.csv\"))\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KAIRkgshwFwP",
   "metadata": {
    "id": "KAIRkgshwFwP",
    "tags": []
   },
   "source": [
    "**Load CNN results as columns to dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Thwg_xPWwJfs",
   "metadata": {
    "id": "Thwg_xPWwJfs"
   },
   "outputs": [],
   "source": [
    "cnn = pd.read_csv(join(config['output'], \"cnn_dataframe.csv\")).drop('Unnamed: 0', axis=1)\n",
    "cnn['CD_UF'] = cnn['CD_UF'].astype(np.int64)\n",
    "\n",
    "assert dataframe.shape[0] == cnn.shape[0]\n",
    "assert all(dataframe['CD_UF'].unique() == cnn['CD_UF'].unique())\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6I4_itCcnn7",
   "metadata": {
    "id": "p6I4_itCcnn7"
   },
   "outputs": [],
   "source": [
    "dataframe.sort_values(['CD_UF', 'Date'], inplace=True, ignore_index=True)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FDTXJ5u_2jLd",
   "metadata": {
    "id": "FDTXJ5u_2jLd"
   },
   "outputs": [],
   "source": [
    "dataframe = pd.concat([dataframe, cnn[['CNN_all', 'CNN_0-19']]], axis=1)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4233a8a-bd01-4edd-96a9-6f8df9733886",
   "metadata": {
    "id": "a4233a8a-bd01-4edd-96a9-6f8df9733886"
   },
   "source": [
    "**'Clean' the dataset (e.g. remove NaN values)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b042e6-c358-4688-b897-e395e312ddc1",
   "metadata": {
    "id": "70b042e6-c358-4688-b897-e395e312ddc1"
   },
   "outputs": [],
   "source": [
    "dataframe = utils.clean(dataframe)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AgWy_v6LJkhM",
   "metadata": {
    "id": "AgWy_v6LJkhM"
   },
   "outputs": [],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ca977-edc7-4d92-9ddb-47c7bd6fbaca",
   "metadata": {
    "id": "ea3ca977-edc7-4d92-9ddb-47c7bd6fbaca"
   },
   "source": [
    "## Apply Data Reduction\n",
    "**Data reduction is applied to three macro groups in order to reduce the number of variables on which the AI framework will be trained. The variables belonging to each group are set with the *PCAgroups* dictionary. The groups are:**\n",
    "1. ***CLIMATIC VARIABLES***,\n",
    "2. ***GEO VARIABLES***,\n",
    "3. ***SOCIO VARIABLES***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953d325-ae5a-4297-adce-b67f5462bb47",
   "metadata": {
    "id": "a953d325-ae5a-4297-adce-b67f5462bb47"
   },
   "outputs": [],
   "source": [
    "print('\\033[1m PCA Excluded Variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['EXCLUDED'])\n",
    "\n",
    "print('\\033[1m Climatic variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['CLIMATIC VARIABLES'])\n",
    "\n",
    "print('\\033[1m Geo variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['GEO VARIABLES'])\n",
    "\n",
    "print('\\033[1m Socio variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['SOCIO VARIABLES'])\n",
    "\n",
    "print('\\033[1m Additional variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['AUXILIAR'])\n",
    "\n",
    "print('\\033[1m Dengue variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['DENGUE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7733c78-84dd-4f71-8b6d-1788c4ea218c",
   "metadata": {
    "id": "b7733c78-84dd-4f71-8b6d-1788c4ea218c"
   },
   "source": [
    "**We selected two types of data reduction methods: PCA (Principal Component Analysis) and PLS (Principal Least Square). The second one is the default solution because it reduces the input data by considering also a second variable that in our case is the Dengue Incidence Rates.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07eec8-4360-4a04-8a1a-9f47e8aadf00",
   "metadata": {
    "id": "ed07eec8-4360-4a04-8a1a-9f47e8aadf00"
   },
   "outputs": [],
   "source": [
    "from data_reduction import pca_reducer, pls_reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493bea11-18e6-49ff-b3db-d749f0812ab9",
   "metadata": {
    "id": "493bea11-18e6-49ff-b3db-d749f0812ab9"
   },
   "source": [
    "**Extract climatic, geophysical and socio-economic variables from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5eef13-eb3e-407c-a44a-7c0d98b476fd",
   "metadata": {
    "id": "5b5eef13-eb3e-407c-a44a-7c0d98b476fd"
   },
   "outputs": [],
   "source": [
    "X_climatic = dataframe[GROUPED_VARS['CLIMATIC VARIABLES']].values\n",
    "X_geo = dataframe[GROUPED_VARS['GEO VARIABLES']].values\n",
    "X_socio = dataframe[GROUPED_VARS['SOCIO VARIABLES']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16023ac4-fc4a-42f5-aaf1-ef6612e443e5",
   "metadata": {
    "id": "16023ac4-fc4a-42f5-aaf1-ef6612e443e5"
   },
   "source": [
    "**Extract Dengue variables from the dataframe, apply a root scaling and normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a51e4-c585-4fc6-9620-3227c699164e",
   "metadata": {
    "id": "816a51e4-c585-4fc6-9620-3227c699164e"
   },
   "outputs": [],
   "source": [
    "y_dengue = dataframe[GROUPED_VARS['DENGUE']].values\n",
    "scaler = MinMaxScaler()\n",
    "y_dengue = scaler.fit_transform(y_dengue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0401b9-59af-4f29-868b-c37052808de9",
   "metadata": {
    "id": "3b0401b9-59af-4f29-868b-c37052808de9"
   },
   "source": [
    "**Apply data reduction technique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b0059-cf6d-469d-ade5-85c86bb5ad36",
   "metadata": {
    "id": "ce6b0059-cf6d-469d-ade5-85c86bb5ad36"
   },
   "outputs": [],
   "source": [
    "if DATA_REDUCER_SETTINGS['TYPE'] == 'PLS':    \n",
    "    climatic_vars_reduced = pls_reducer(\n",
    "        X_climatic,\n",
    "        y_dengue,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['CLIMATIC VARIABLES'])\n",
    "    \n",
    "    geo_vars_reduced = pls_reducer(\n",
    "        X_geo,\n",
    "        y_dengue,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['GEO VARIABLES'])\n",
    "    \n",
    "    socio_vars_reduced = pls_reducer(\n",
    "        X_socio,\n",
    "        y_dengue,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['SOCIO VARIABLES'])\n",
    "    \n",
    "elif DATA_REDUCER_SETTINGS['TYPE'] == 'PCA':\n",
    "    climatic_vars_reduced = pca_reducer(\n",
    "        X_climatic,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['CLIMATIC VARIABLES'])\n",
    "    \n",
    "    geo_vars_reduced = pca_reducer(\n",
    "        X_geo,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['GEO VARIABLES'])\n",
    "    \n",
    "    socio_vars_reduced = pca_reducer(\n",
    "        X_socio,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['SOCIO VARIABLES'])\n",
    "else:\n",
    "    print('No data reduction.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ae33ce-0228-4303-a796-ce8a7c50869a",
   "metadata": {
    "id": "52ae33ce-0228-4303-a796-ce8a7c50869a"
   },
   "source": [
    "## Order reduced data in a new dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c063e-4ae5-4371-98ac-def71760765b",
   "metadata": {
    "id": "979c063e-4ae5-4371-98ac-def71760765b"
   },
   "source": [
    "**Normalize remaining variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zYOskxhcmUDF",
   "metadata": {
    "id": "zYOskxhcmUDF"
   },
   "outputs": [],
   "source": [
    "x_excluded = dataframe[GROUPED_VARS['EXCLUDED']].values\n",
    "x_excluded = MinMaxScaler().fit_transform(x_excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543c83f-80b6-4d4a-9ff6-7188a7ce7b63",
   "metadata": {
    "id": "b543c83f-80b6-4d4a-9ff6-7188a7ce7b63"
   },
   "outputs": [],
   "source": [
    "X_auxiliar = dataframe[GROUPED_VARS['AUXILIAR']].values\n",
    "X_auxiliar = MinMaxScaler().fit_transform(X_auxiliar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3da58-db5b-456f-a119-0830ad336ced",
   "metadata": {
    "id": "39c3da58-db5b-456f-a119-0830ad336ced"
   },
   "source": [
    "**Create a new database with the reduced, the auxiliar and Dengue variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df37d0b-af3d-4ad6-b17d-5f9f6317f6ad",
   "metadata": {
    "id": "1df37d0b-af3d-4ad6-b17d-5f9f6317f6ad"
   },
   "outputs": [],
   "source": [
    "independent = {'Year':dataframe.Year.values, 'dep_id':dataframe.CD_UF.values, 't_fundc_ocup18m':x_excluded[:, 0], 't_medioc_ocup18m':x_excluded[:, 1],\n",
    "               'PopTotal_Urban_UF':x_excluded[:, 2], 'PopTotal_Rural_UF':x_excluded[:, 3], 'total_precipitation_d':x_excluded[:, 4], \n",
    "               'surface_pressure_d':x_excluded[:, 5], 'area_km2':x_excluded[:, 6], 'humidity_d':x_excluded[:, 7], 'temperature_2m_d':x_excluded[:, 8],\n",
    "               'min_temperature_2m_d':x_excluded[:, 9], 'CNN_all':x_excluded[:, 10], 'CNN_0-19':x_excluded[:, 11]}\n",
    "\n",
    "auxiliar    = {'Month': X_auxiliar[:, 0], \n",
    "               'cases20_99': X_auxiliar[:, 1], 'cases0_19': X_auxiliar[:, 2],\n",
    "               'RandEffects1':  MinMaxScaler().fit_transform(np.reshape(dataframe.CD_UF.values*dataframe.Month.values, (dataframe.CD_UF.values.shape[0], 1)))[:,0], \n",
    "               'RandEffects2':  MinMaxScaler().fit_transform(np.reshape(dataframe.CD_UF.values*dataframe.Year.values, (dataframe.CD_UF.values.shape[0], 1)))[:,0], \n",
    "               'RandEffects3':  MinMaxScaler().fit_transform(np.reshape(dataframe.CD_UF.values*dataframe.Month.values*dataframe.Year.values, (dataframe.CD_UF.values.shape[0], 1)))[:,0]}\n",
    "\n",
    "climatic    = {'PCA0-Climatic':climatic_vars_reduced[:,0], 'PCA1-Climatic':climatic_vars_reduced[:,1], 'PCA2-Climatic':climatic_vars_reduced[:,2], \n",
    "               'PCA3-Climatic':climatic_vars_reduced[:,3]}\n",
    "\n",
    "geo         = {'PCA0-Geo':geo_vars_reduced[:,0], 'PCA1-Geo':geo_vars_reduced[:,1], 'PCA2-Geo':geo_vars_reduced[:,2], \n",
    "               'PCA3-Geo':geo_vars_reduced[:,3], 'PCA4-Geo':geo_vars_reduced[:,4], 'PCA5-Geo':geo_vars_reduced[:,5]}\n",
    "\n",
    "socio       = {'PCA0-Socio':socio_vars_reduced[:,0], 'PCA1-Socio':socio_vars_reduced[:,1], 'PCA2-Socio':socio_vars_reduced[:,2], \n",
    "               'PCA3-Socio':socio_vars_reduced[:,3], 'PCA4-Socio':socio_vars_reduced[:,4], 'PCA5-Socio':socio_vars_reduced[:,5]}\n",
    "\n",
    "dengue      = {'DengRate_all': y_dengue[:,0], 'DengRate_019': y_dengue[:,1]}\n",
    "\n",
    "columns     = {**independent, **auxiliar, **climatic, **geo, **socio, **dengue}\n",
    "\n",
    "reduced_dataframe = pd.DataFrame(columns)\n",
    "reduced_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462125c4-fbb2-4fe7-8cfd-3e023fd5356c",
   "metadata": {
    "id": "462125c4-fbb2-4fe7-8cfd-3e023fd5356c"
   },
   "source": [
    "## Create training and validation data\n",
    "**First of all, the dataframe is divided in two sub-dataframes (training and validation) by using the variable *Year***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb37c08d-4df8-4843-8a40-54877433d299",
   "metadata": {
    "id": "fb37c08d-4df8-4843-8a40-54877433d299"
   },
   "outputs": [],
   "source": [
    "training_dataframe   = reduced_dataframe[reduced_dataframe.Year <= 2016]\n",
    "validation_dataframe = reduced_dataframe[reduced_dataframe.Year >= 2016]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c096a133-8ecc-479c-a82b-7c4b5ebf0558",
   "metadata": {
    "id": "c096a133-8ecc-479c-a82b-7c4b5ebf0558"
   },
   "source": [
    "**Then the dataset handler is initialized. This object will handle all the operations needed to create, reshape and augment the training and validation dataset to fit the requirements of each Deep Learning or Machine Learning model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180107d5-4541-4cdc-a6df-6ce7e272bddd",
   "metadata": {
    "id": "180107d5-4541-4cdc-a6df-6ce7e272bddd"
   },
   "outputs": [],
   "source": [
    "from datasetHandler import datasetHandler\n",
    "dataset_handler = datasetHandler(training_dataframe, validation_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605b584-8866-4792-acdb-17dfcd613d0d",
   "metadata": {
    "id": "8605b584-8866-4792-acdb-17dfcd613d0d"
   },
   "source": [
    "**Get training and validation vectors from dataframes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b39bd-1397-4376-bd84-06fa3ffadd77",
   "metadata": {
    "id": "f79b39bd-1397-4376-bd84-06fa3ffadd77"
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val = dataset_handler.get_data(DATA_PROCESSING_SETTINGS['T LEARNING'], DATA_PROCESSING_SETTINGS['T PREDICTION'])\n",
    "print('\\n\\nX Training shape', x_train.shape)\n",
    "print('Y Training shape', y_train.shape)\n",
    "print('X Validation shape', x_val.shape)\n",
    "print('Y Validation shape', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164e07a3-8236-491c-9c10-6c270cc1d83e",
   "metadata": {
    "id": "164e07a3-8236-491c-9c10-6c270cc1d83e"
   },
   "source": [
    "**Apply data augmention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936103f-fe82-45d0-9ffa-e404d61b96a6",
   "metadata": {
    "id": "2936103f-fe82-45d0-9ffa-e404d61b96a6"
   },
   "outputs": [],
   "source": [
    "x_train_a, y_train_a, x_val_a, y_val_a = dataset_handler.augment(x_train, y_train, x_val, y_val, DATA_PROCESSING_SETTINGS['AUGMENTATION'])\n",
    "print('X Training shape', x_train_a.shape)\n",
    "print('Y Training shape', y_train_a.shape)\n",
    "print('X Validation shape', x_val_a.shape)\n",
    "print('Y Validation shape', y_val_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EG-TYA2Ispib",
   "metadata": {
    "id": "EG-TYA2Ispib"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80-UoybcjRp",
   "metadata": {
    "id": "c80-UoybcjRp"
   },
   "source": [
    "## Baseline\n",
    "**Upload results from Baseline Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FXbD73W0cr5L",
   "metadata": {
    "id": "FXbD73W0cr5L"
   },
   "outputs": [],
   "source": [
    "baselineAll = pd.read_csv(join(config['baseline'], \"Brazil\", \"Baseline_fitted_predicted_AllCases.csv\"))\n",
    "baselineAll = baselineAll[['Date','Year','Month','CD_UF','PopTotal_UF','state_index','pred.uci','pred.mean','pred.lci']]\n",
    "baselineAll['state_index'] -= 1\n",
    "baselineAll['Date'] = pd.to_datetime(baselineAll['Date'])\n",
    "\n",
    "cols = ['pred.uci','pred.mean','pred.lci']\n",
    "baselineAll.loc[:, cols] = baselineAll.loc[:, cols].div(baselineAll['PopTotal_UF'], axis=0).multiply(100000.0, axis=0)\n",
    "baselineAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CIT_6Lieq6y4",
   "metadata": {
    "id": "CIT_6Lieq6y4"
   },
   "outputs": [],
   "source": [
    "baseline019 = pd.read_csv(join(config['baseline'], \"Brazil\", \"Baseline_fitted_predicted_0_19cases.csv\"))\n",
    "baseline019 = baseline019[['Date','Year','Month','CD_UF','Pop0_19_UF','state_index','pred.uci','pred.mean','pred.lci']]\n",
    "baseline019['state_index'] -= 1\n",
    "baseline019['Date'] = pd.to_datetime(baseline019['Date'])\n",
    "\n",
    "cols = ['pred.uci','pred.mean','pred.lci']\n",
    "baseline019.loc[:, cols] = baseline019.loc[:, cols].div(baseline019['Pop0_19_UF'], axis=0).multiply(100000.0, axis=0)\n",
    "baseline019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a956d2f-f784-4d7e-953d-dfac8c36b3ba",
   "metadata": {
    "id": "6a956d2f-f784-4d7e-953d-dfac8c36b3ba",
    "tags": []
   },
   "source": [
    "## CatBoost\n",
    "**Prepare dataset to fit CatBoost requirements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4223e056-f0e6-4f7d-a64a-6e2312c423ad",
   "metadata": {
    "id": "4223e056-f0e6-4f7d-a64a-6e2312c423ad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingC, validationC = dataset_handler.prepare_data_CatBoost(x_train_a[:,:,2:], y_train_a, x_val_a[:,:,2:], y_val_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7656808-1dc4-4ee2-b4b9-dafd208302bd",
   "metadata": {
    "id": "c7656808-1dc4-4ee2-b4b9-dafd208302bd"
   },
   "source": [
    "**Initilize the CatBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ec7e36-3097-4027-9689-fce5aa650ea6",
   "metadata": {
    "id": "d9ec7e36-3097-4027-9689-fce5aa650ea6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models import CatBoostNet\n",
    "cat_boost = CatBoostNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d2393-baca-492a-9b49-54d7b71f57ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_boost.model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb2420-a30c-44cd-bc0c-cc4dde58a263",
   "metadata": {
    "id": "25bb2420-a30c-44cd-bc0c-cc4dde58a263"
   },
   "source": [
    "**Train or Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OpXZZV2VU0va",
   "metadata": {
    "id": "OpXZZV2VU0va",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_scores(gt, preds):\n",
    "    # Compute Normalized-RMSE and R2 metrics\n",
    "    rmse = mean_squared_error(gt, preds, squared=False)\n",
    "    nrmse = rmse/(gt.max() - gt.min())\n",
    "    r2 = r2_score(gt, preds)\n",
    "    return nrmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ad018-27ff-45fc-8f3d-f2bb92bb2c82",
   "metadata": {
    "id": "d87ad018-27ff-45fc-8f3d-f2bb92bb2c82",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "nb_deps = 27\n",
    "\n",
    "if TRAINING:\n",
    "    cat_boost.train(trainingC, validationC, output_path=join(config['output'], \"Brazil\"))\n",
    "\n",
    "else:\n",
    "    # get most recent catboost model\n",
    "    cat_boost_model = glob(join(config['output'], \"Brazil\", \"catboost*\"))\n",
    "    if cat_boost_model == []:\n",
    "        print('No file with such pattern was found in the directory. Run TRAINING = True first.')\n",
    "    else:\n",
    "        cat_boost.load(cat_boost_model[0])\n",
    "\n",
    "        trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "        preds_tra = cat_boost.model.predict(trainC[0])\n",
    "        preds_tra[preds_tra < 0] = 0\n",
    "        preds_val = cat_boost.model.predict(valC[0])\n",
    "        preds_val[preds_val < 0] = 0\n",
    "\n",
    "        datelist = pd.date_range('01-01-2001', end='01-01-2020', freq='YS')\n",
    "        dates = datelist.strftime(\"%Y\").tolist()\n",
    "\n",
    "        # Total timesteps (nb of months inside this time window) = 228\n",
    "        month_datelist = pd.date_range('01-01-2001', end='31-12-2019', freq='MS')\n",
    "        ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "        res = []\n",
    "        res.append('Department,NRMSE 0-19 Training,NRMSE All Training,NRMSE 0-19 Validation,NRMSE All Validation')\n",
    "\n",
    "        for i in range(nb_deps):\n",
    "            fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18,9))\n",
    "\n",
    "            idxt = preds_tra.shape[0]//nb_deps\n",
    "            idxv = preds_val.shape[0]//nb_deps\n",
    "\n",
    "            t1 = ts[12:12+idxt]\n",
    "            t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "            # Reverting Data Norm ---------\n",
    "            pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "            pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "            gtt = scaler.inverse_transform(trainC[1][idxt*i:idxt*(i+1), :])\n",
    "            gtv = scaler.inverse_transform(valC[1][idxv*i:idxv*(i+1), :])\n",
    "            # ------------------------------\n",
    "\n",
    "            baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "            base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "            axes[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "            axes[0].plot(t1, pt[:,0], '--', color='red', label = 'CatBoost prediction on Training data')\n",
    "            axes[0].plot(t2, pv[:,0], '-', color='red', label = 'CatBoost prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            axes[1].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "            axes[1].plot(t1, pt[:,1], '--', color='red', label = 'CatBoost prediction on Training data')\n",
    "            axes[1].plot(t2, pv[:,1], '-', color='red', label = 'CatBoost prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[1].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[1].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            # Configure visualization\n",
    "            axes[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left')\n",
    "            axes[0].set(ylabel='DIR Total Population')\n",
    "            axes[1].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "            axes[1].legend(ncol=3, loc='lower center', bbox_to_anchor=(0.5,-0.5), shadow=True)\n",
    "            axes[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "            axes[1].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "            for ax in axes:\n",
    "                ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "                ax.grid(True, linewidth=0.5)\n",
    "                ax.set_xlim(ts[0],ts[-1]+5)\n",
    "            axes[0].set_xticklabels([])\n",
    "            axes[1].set_xticklabels(dates)\n",
    "\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            print('--------------------- TRAINING scores ---------------------')\n",
    "            rmse1, r2 = compute_scores(gtt[:,0], pt[:,0])\n",
    "            print('N-RMSE (group total): ', rmse1)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse2, r2 = compute_scores(gtt[:,1], pt[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse2)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            print('-------------------- VALIDATION scores ---------------------')\n",
    "            rmse3, r2 = compute_scores(gtv[:,0], pv[:,0])\n",
    "            print('N-RMSE (group total): ', rmse3)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse4, r2 = compute_scores(gtv[:,1], pv[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse4)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            res.append('{},{},{},{},{}'.format(DEP_NAMES[i], rmse2, rmse1, rmse4, rmse3))\n",
    "            print('\\n##########################################################################################################')\n",
    "\n",
    "        # Save results\n",
    "        today = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
    "        with open(join(config['metrics'], \"Brazil\", 'catboost-'+today+'.csv'),'w') as f:\n",
    "            print('Writing performance metrics to file...')\n",
    "            for item in res:\n",
    "                f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Le8KHGbJ2MuH",
   "metadata": {
    "id": "Le8KHGbJ2MuH"
   },
   "source": [
    "Write file with performance metrics for baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oJpCRde21sFh",
   "metadata": {
    "id": "oJpCRde21sFh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_boost_model = glob(join(config['output'], \"Brazil\", \"catboost*\"))[0]\n",
    "cat_boost.load(cat_boost_model)\n",
    "\n",
    "trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "preds_tra = cat_boost.model.predict(trainC[0])\n",
    "preds_tra[preds_tra < 0] = 0\n",
    "preds_val = cat_boost.model.predict(valC[0])\n",
    "preds_val[preds_val < 0] = 0\n",
    "\n",
    "datelist = pd.date_range('01-01-2001', end='01-01-2020', freq='YS')\n",
    "dates = datelist.strftime(\"%Y\").tolist()\n",
    "# Total timesteps (nb of months inside this time window) = 228\n",
    "month_datelist = pd.date_range('01-01-2001', end='31-12-2019', freq='MS')\n",
    "ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "res = []\n",
    "res.append('Department,NRMSE 0-19 Training,NRMSE All Training,NRMSE 0-19 Validation,NRMSE All Validation')\n",
    "\n",
    "for i in range(nb_deps):\n",
    "    idxt = preds_tra.shape[0]//nb_deps\n",
    "    idxv = preds_val.shape[0]//nb_deps \n",
    "\n",
    "    t1 = ts[12:12+idxt]\n",
    "    t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "    # Reverting Data Norm ---------\n",
    "    gtt = scaler.inverse_transform(trainC[1][idxt*i:idxt*(i+1), :])\n",
    "    gtv = scaler.inverse_transform(valC[1][idxv*i:idxv*(i+1), :])\n",
    "    # ------------------------------\n",
    "    \n",
    "    baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "    base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "    print('--------------------- TRAINING scores ---------------------')\n",
    "    rmse1, r2 = compute_scores(gtt[:,0], baseAll_df['pred.mean'].iloc[t1])\n",
    "    print('N-RMSE (group total): ', rmse1)\n",
    "    print('R2   (group total): ', r2)\n",
    "\n",
    "    rmse2, r2 = compute_scores(gtt[:,1], base019_df['pred.mean'].iloc[t1])\n",
    "    print('N-RMSE (group 0-19): ', rmse2)\n",
    "    print('R2   (group 0-19): ', r2)\n",
    "\n",
    "    print('-------------------- VALIDATION scores ---------------------')\n",
    "    rmse3, r2 = compute_scores(gtv[:,0], baseAll_df['pred.mean'].iloc[t2])\n",
    "    print('N-RMSE (group total): ', rmse3)\n",
    "    print('R2   (group total): ', r2)\n",
    "\n",
    "    rmse4, r2 = compute_scores(gtv[:,1], base019_df['pred.mean'].iloc[t2])\n",
    "    print('N-RMSE (group 0-19): ', rmse4)\n",
    "    print('R2   (group 0-19): ', r2)\n",
    "\n",
    "    res.append('{},{},{},{},{}'.format(DEP_NAMES[i], rmse2, rmse1, rmse4, rmse3))\n",
    "    \n",
    "    print('\\n##########################################################################################################')\n",
    "\n",
    "# Save results\n",
    "today = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
    "with open(join(config['metrics'], \"Brazil\", 'baseline-'+today+'.csv'),'w') as f:\n",
    "    print('Writing performance metrics to file...')\n",
    "    for item in res:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E3iGL37YB9FU",
   "metadata": {
    "id": "E3iGL37YB9FU"
   },
   "source": [
    "## SVM\n",
    "**It uses the same data structure of CatBoost, no need to prepare data**\n",
    "\n",
    "**Initilize the SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P0TfzBbeB8yl",
   "metadata": {
    "id": "P0TfzBbeB8yl"
   },
   "outputs": [],
   "source": [
    "from models import SVMNet\n",
    "svm = SVMNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mho-Gh3HDan5",
   "metadata": {
    "id": "mho-Gh3HDan5"
   },
   "source": [
    "**Train or Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZXFfioFlC9-9",
   "metadata": {
    "id": "ZXFfioFlC9-9"
   },
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "\n",
    "if TRAINING:\n",
    "    svm.train(trainingC, validationC, output_path=join(config['output'], \"Brazil\"))\n",
    "\n",
    "else:\n",
    "    # get most recent svm model\n",
    "    svm_model = glob(join(config['output'], \"Brazil\", \"svm*\"))\n",
    "    if svm_model == []:\n",
    "        print('No file with such pattern was found in the directory. Run TRAINING = True first.')\n",
    "    else:\n",
    "        svm.load(svm_model[0])\n",
    "\n",
    "        trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "        preds_tra = svm.model.predict(trainC[0])\n",
    "        preds_tra[preds_tra < 0] = 0\n",
    "        preds_val = svm.model.predict(valC[0])\n",
    "        preds_val[preds_val < 0] = 0\n",
    "\n",
    "        datelist = pd.date_range('01-01-2001', end='01-01-2020', freq='YS')\n",
    "        dates = datelist.strftime(\"%Y\").tolist()\n",
    "        month_datelist = pd.date_range('01-01-2001', end='31-12-2019', freq='MS')\n",
    "        ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "        res = []\n",
    "        res.append('Department,NRMSE 0-19 Training,NRMSE All Training,NRMSE 0-19 Validation,NRMSE All Validation')\n",
    "\n",
    "        for i in range(nb_deps):\n",
    "            fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18,9))\n",
    "\n",
    "            idxt = preds_tra.shape[0]//nb_deps\n",
    "            idxv = preds_val.shape[0]//nb_deps\n",
    "\n",
    "            t1 = ts[12:12+idxt]\n",
    "            t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "            # Reverting Data Norm ---------\n",
    "            pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "            pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "            gtt = scaler.inverse_transform(trainC[1][idxt*i:idxt*(i+1), :])\n",
    "            gtv = scaler.inverse_transform(valC[1][idxv*i:idxv*(i+1), :])\n",
    "            # ------------------------------\n",
    "\n",
    "            baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "            base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "            axes[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "            axes[0].plot(t1, pt[:,0], '--', color='red', label = 'SVM prediction on Training data')\n",
    "            axes[0].plot(t2, pv[:,0], '-', color='red', label = 'SVM prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            axes[1].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "            axes[1].plot(t1, pt[:,1], '--', color='red', label = 'SVM prediction on Training data')\n",
    "            axes[1].plot(t2, pv[:,1], '-', color='red', label = 'SVM prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[1].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[1].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            # Configure visualization\n",
    "            axes[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left')\n",
    "            axes[0].set(ylabel='DIR Total Population')\n",
    "            axes[1].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "            axes[1].legend(ncol=3, loc='lower center', bbox_to_anchor=(0.5,-0.5), shadow=True)\n",
    "            axes[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "            axes[1].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "            for ax in axes:\n",
    "                ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "                ax.grid(True, linewidth=0.5)\n",
    "                ax.set_xlim(ts[0],ts[-1]+5)\n",
    "            axes[0].set_xticklabels([])\n",
    "            axes[1].set_xticklabels(dates)\n",
    "\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            print('--------------------- TRAINING scores ---------------------')\n",
    "            rmse1, r2 = compute_scores(gtt[:,0], pt[:,0])\n",
    "            print('N-RMSE (group total): ', rmse1)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse2, r2 = compute_scores(gtt[:,1], pt[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse2)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            print('-------------------- VALIDATION scores ---------------------')\n",
    "            rmse3, r2 = compute_scores(gtv[:,0], pv[:,0])\n",
    "            print('N-RMSE (group total): ', rmse3)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse4, r2 = compute_scores(gtv[:,1], pv[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse4)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            res.append('{},{},{},{},{}'.format(DEP_NAMES[i], rmse2, rmse1, rmse4, rmse3))\n",
    "            print('\\n##########################################################################################################')\n",
    "\n",
    "        # Save results\n",
    "        today = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
    "        with open(join(config['metrics'], \"Brazil\", 'svm-'+today+'.csv'),'w') as f:\n",
    "            print('Writing performance metrics to file...')\n",
    "            for item in res:\n",
    "                f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nhxwEojupvjg",
   "metadata": {
    "id": "nhxwEojupvjg",
    "tags": []
   },
   "source": [
    "## LSTM\n",
    "**Prepare dataset to fit LSTM requirements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WxW7x16epvji",
   "metadata": {
    "id": "WxW7x16epvji"
   },
   "outputs": [],
   "source": [
    "trainingL, validationL = dataset_handler.prepare_data_LSTM(x_train_a[:,:,2:], y_train_a, x_val_a[:,:,2:], y_val_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZMlQADOWpvjl",
   "metadata": {
    "id": "ZMlQADOWpvjl"
   },
   "source": [
    "**Initilize the LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "En62JMKxpvjo",
   "metadata": {
    "id": "En62JMKxpvjo"
   },
   "outputs": [],
   "source": [
    "from models import LSTMNet\n",
    "lstm = LSTMNet(trainingL[0].shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wBlGgF1Npvjq",
   "metadata": {
    "id": "wBlGgF1Npvjq"
   },
   "source": [
    "**Train or Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OX-zrXn8pvjs",
   "metadata": {
    "id": "OX-zrXn8pvjs"
   },
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "\n",
    "if TRAINING:\n",
    "    lstm.train(trainingL, validationL, output_path=join(config['output'], \"Brazil\"))\n",
    "\n",
    "else:\n",
    "    # get most recent lstm model\n",
    "    lstm_model = glob(join(config['output'], \"Brazil\", \"lstm*\"))\n",
    "    if lstm_model == []:\n",
    "        print('No file with such pattern was found in the directory. Run TRAINING = True first.')\n",
    "    else:\n",
    "        lstm.load(lstm_model[0])\n",
    "\n",
    "        trainL, valL = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "        preds_tra = lstm.model.predict(trainL[0])\n",
    "        preds_tra[preds_tra < 0] = 0\n",
    "        preds_val = lstm.model.predict(valL[0])\n",
    "        preds_val[preds_val < 0] = 0\n",
    "\n",
    "        datelist = pd.date_range('01-01-2001', end='01-01-2020', freq='YS')\n",
    "        dates = datelist.strftime(\"%Y\").tolist()\n",
    "        month_datelist = pd.date_range('01-01-2001', end='31-12-2019', freq='MS')\n",
    "        ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "        res = []\n",
    "        res.append('Department,NRMSE 0-19 Training,NRMSE All Training,NRMSE 0-19 Validation,NRMSE All Validation')\n",
    "\n",
    "        for i in range(nb_deps):\n",
    "            fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18,9))\n",
    "\n",
    "            idxt = preds_tra.shape[0]//nb_deps\n",
    "            idxv = preds_val.shape[0]//nb_deps\n",
    "\n",
    "            t1 = ts[12:12+idxt]\n",
    "            t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "            # Reverting Data Norm ---------\n",
    "            pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "            pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "            gtt = scaler.inverse_transform(trainL[1][idxt*i:idxt*(i+1), :])\n",
    "            gtv = scaler.inverse_transform(valL[1][idxv*i:idxv*(i+1), :])\n",
    "            # ------------------------------\n",
    "\n",
    "            baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "            base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "            axes[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "            axes[0].plot(t1, pt[:,0], '--', color='red', label = 'LSTM prediction on Training data')\n",
    "            axes[0].plot(t2, pv[:,0], '-', color='red', label = 'LSTM prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            axes[1].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "            axes[1].plot(t1, pt[:,1], '--', color='red', label = 'LSTM prediction on Training data')\n",
    "            axes[1].plot(t2, pv[:,1], '-', color='red', label = 'LSTM prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[1].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[1].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            # Configure visualization\n",
    "            axes[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left')\n",
    "            axes[0].set(ylabel='DIR Total Population')\n",
    "            axes[1].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "            axes[1].legend(ncol=3, loc='lower center', bbox_to_anchor=(0.5,-0.5), shadow=True)\n",
    "            axes[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "            axes[1].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "            for ax in axes:\n",
    "                ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "                ax.grid(True, linewidth=0.5)\n",
    "                ax.set_xlim(ts[0],ts[-1]+5)\n",
    "            axes[0].set_xticklabels([])\n",
    "            axes[1].set_xticklabels(dates)\n",
    "\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            print('--------------------- TRAINING scores ---------------------')\n",
    "            rmse1, r2 = compute_scores(gtt[:,0], pt[:,0])\n",
    "            print('N-RMSE (group total): ', rmse1)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse2, r2 = compute_scores(gtt[:,1], pt[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse2)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            print('-------------------- VALIDATION scores ---------------------')\n",
    "            rmse3, r2 = compute_scores(gtv[:,0], pv[:,0])\n",
    "            print('N-RMSE (group total): ', rmse3)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse4, r2 = compute_scores(gtv[:,1], pv[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse4)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            res.append('{},{},{},{},{}'.format(DEP_NAMES[i], rmse2, rmse1, rmse4, rmse3))\n",
    "            print('\\n##########################################################################################################')\n",
    "\n",
    "        # Save results\n",
    "        today = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
    "        with open(join(config['metrics'], \"Brazil\", 'lstm-'+today+'.csv'),'w') as f:\n",
    "            print('Writing performance metrics to file...')\n",
    "            for item in res:\n",
    "                f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oAK1FjNaPrpZ",
   "metadata": {
    "id": "oAK1FjNaPrpZ",
    "tags": []
   },
   "source": [
    "## Ensemble\n",
    "**Prepare dataset to fit the Ensemble requirements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O0dFRcYMhCDP",
   "metadata": {
    "id": "O0dFRcYMhCDP"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd115599-acec-403e-94f0-9f708eb0d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(trainL, valL):\n",
    "\n",
    "    def pol_zero(x, a):\n",
    "        return a \n",
    "\n",
    "    def pol_one(x, a, b):\n",
    "        return a + b*x\n",
    "\n",
    "    def pol_two(x, a, b, c):\n",
    "        return a + b*x + c*x*x  \n",
    "\n",
    "    train_019_pol0 = []\n",
    "    train_019_pol1 = []\n",
    "    train_019_pol2 = []\n",
    "\n",
    "    train_all_pol0 = []\n",
    "    train_all_pol1 = []\n",
    "    train_all_pol2 = []\n",
    "\n",
    "    val_019_pol0 = []\n",
    "    val_019_pol1 = []\n",
    "    val_019_pol2 = []\n",
    "\n",
    "    val_all_pol0 = []\n",
    "    val_all_pol1 = []\n",
    "    val_all_pol2 = []\n",
    "\n",
    "    for i in tqdm(range(trainL[0].shape[0])):\n",
    "        popt_0, _ = curve_fit(pol_zero, np.arange(0,12,1), trainL[0][i,:,-1])\n",
    "        popt_1, _ = curve_fit(pol_one, np.arange(0,12,1),  trainL[0][i,:,-1])\n",
    "        popt_2, _ = curve_fit(pol_two, np.arange(0,12,1), trainL[0][i,:,-1])\n",
    "\n",
    "        train_019_pol0.append(pol_zero(12, *popt_0))\n",
    "        train_019_pol1.append(pol_one(12, *popt_1))\n",
    "        train_019_pol2.append(pol_two(12, *popt_2))\n",
    "\n",
    "        popt_0, _ = curve_fit(pol_zero, np.arange(0,12,1), trainL[0][i,:,-2])\n",
    "        popt_1, _ = curve_fit(pol_one, np.arange(0,12,1),  trainL[0][i,:,-2])\n",
    "        popt_2, _ = curve_fit(pol_two, np.arange(0,12,1),  trainL[0][i,:,-2])\n",
    "\n",
    "        train_all_pol0.append(pol_zero(12, *popt_0))\n",
    "        train_all_pol1.append(pol_one(12, *popt_1))\n",
    "        train_all_pol2.append(pol_two(12, *popt_2))\n",
    "\n",
    "    for i in tqdm(range(valL[0].shape[0])):\n",
    "        popt_0, _ = curve_fit(pol_zero, np.arange(0,12,1), valL[0][i,:,-1])\n",
    "        popt_1, _ = curve_fit(pol_one, np.arange(0,12,1), valL[0][i,:,-1])\n",
    "        popt_2, _ = curve_fit(pol_two, np.arange(0,12,1), valL[0][i,:,-1])\n",
    "\n",
    "        val_019_pol0.append(pol_zero(12, *popt_0))\n",
    "        val_019_pol1.append(pol_one(12, *popt_1))\n",
    "        val_019_pol2.append(pol_two(12, *popt_2))\n",
    "\n",
    "        popt_0, _ = curve_fit(pol_zero, np.arange(0,12,1), valL[0][i,:,-2])\n",
    "        popt_1, _ = curve_fit(pol_one, np.arange(0,12,1), valL[0][i,:,-2])\n",
    "        popt_2, _ = curve_fit(pol_two, np.arange(0,12,1), valL[0][i,:,-2])\n",
    "\n",
    "        val_all_pol0.append(pol_zero(12, *popt_0))\n",
    "        val_all_pol1.append(pol_one(12, *popt_1))\n",
    "        val_all_pol2.append(pol_two(12, *popt_2))\n",
    "\n",
    "    train_019_pol0 = np.array(train_019_pol0)\n",
    "    train_019_pol1 = np.array(train_019_pol1)\n",
    "    train_019_pol2 = np.array(train_019_pol2)\n",
    "\n",
    "    train_all_pol0 = np.array(train_all_pol0)\n",
    "    train_all_pol1 = np.array(train_all_pol1)\n",
    "    train_all_pol2 = np.array(train_all_pol2)\n",
    "\n",
    "    val_019_pol0 = np.array(val_019_pol0)\n",
    "    val_019_pol1 = np.array(val_019_pol1)\n",
    "    val_019_pol2 = np.array(val_019_pol2)\n",
    "\n",
    "    val_all_pol0 = np.array(val_all_pol0)\n",
    "    val_all_pol1 = np.array(val_all_pol1)\n",
    "    val_all_pol2 = np.array(val_all_pol2)\n",
    "\n",
    "\n",
    "    train_pol0 = np.column_stack((train_019_pol0, train_all_pol0))\n",
    "    train_pol1 = np.column_stack((train_019_pol1, train_all_pol1))\n",
    "    train_pol2 = np.column_stack((train_019_pol2, train_all_pol2))\n",
    "\n",
    "    val_pol0 = np.column_stack((val_019_pol0, val_all_pol0))\n",
    "    val_pol1 = np.column_stack((val_019_pol1, val_all_pol1))\n",
    "    val_pol2 = np.column_stack((val_019_pol2, val_all_pol2))\n",
    "\n",
    "    return train_pol0, train_pol1, train_pol2, val_pol0, val_pol1, val_pol2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9EhJ44gxPrpb",
   "metadata": {
    "id": "9EhJ44gxPrpb",
    "tags": []
   },
   "source": [
    "**Initilize the Ensemble model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GX4eBGHH0LKS",
   "metadata": {
    "id": "GX4eBGHH0LKS"
   },
   "outputs": [],
   "source": [
    "from models import Ensamble, CatBoostEnsableNet, RandomForestEnsableNet\n",
    "ens = RandomForestEnsableNet(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waM5x7G4Prpe",
   "metadata": {
    "id": "waM5x7G4Prpe"
   },
   "source": [
    "**Train or Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QeQEC7APXM2w",
   "metadata": {
    "id": "QeQEC7APXM2w"
   },
   "outputs": [],
   "source": [
    "# Load the inner models\n",
    "cat_boost.load(glob(join(config['output'], \"Brazil\", \"catboost*\"))[0])\n",
    "svm.load(glob(join(config['output'], \"Brazil\", \"svm*\"))[0])\n",
    "lstm.load(glob(join(config['output'], \"Brazil\", \"lstm*\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oKKkq04-Prpf",
   "metadata": {
    "id": "oKKkq04-Prpf"
   },
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "\n",
    "if TRAINING:\n",
    "\n",
    "    catBoost_train = cat_boost.model.predict(trainingC[0])\n",
    "    catBoost_train[catBoost_train < 0] = 0\n",
    "    catBoost_val = cat_boost.model.predict(validationC[0])\n",
    "    catBoost_val[catBoost_val < 0] = 0\n",
    "\n",
    "    svm_train = svm.model.predict(trainingC[0])\n",
    "    svm_train[svm_train < 0] = 0\n",
    "    svm_val = svm.model.predict(validationC[0])\n",
    "    svm_val[svm_val < 0] = 0\n",
    "\n",
    "    lstm_train = lstm.model.predict(trainingL[0])\n",
    "    lstm_train[lstm_train < 0] = 0\n",
    "    lstm_val = lstm.model.predict(validationL[0])\n",
    "    lstm_val[lstm_val < 0] = 0\n",
    "\n",
    "    train_pol0, train_pol1, train_pol2, val_pol0, val_pol1, val_pol2 = interpolation(trainingL, validationL)\n",
    "\n",
    "    x_trainE = np.concatenate((catBoost_train, svm_train, lstm_train, train_pol0, train_pol1, train_pol2), axis=-1)\n",
    "    y_trainE = trainingL[1]\n",
    "    x_valE = np.concatenate((catBoost_val, svm_val, lstm_val, val_pol0, val_pol1, val_pol2), axis=-1)\n",
    "    y_valE = validationL[1]\n",
    "\n",
    "    ens.train(x_trainE, y_trainE, x_valE, y_valE, output_path=join(config['output'], \"Brazil\"))\n",
    "\n",
    "else:\n",
    "    # Load the ensemble model\n",
    "    ens_model = glob(join(config['output'], \"Brazil\", \"rf*\"))\n",
    "    if ens_model == []:\n",
    "        print('No file with such pattern was found in the directory. Run TRAINING = True first.')\n",
    "    else:\n",
    "        ens.load(ens_model[0])\n",
    "\n",
    "        trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "        trainL, valL = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "        catBoost_train = cat_boost.model.predict(trainC[0])\n",
    "        catBoost_train[catBoost_train < 0] = 0\n",
    "        catBoost_val = cat_boost.model.predict(valC[0])\n",
    "        catBoost_val[catBoost_val < 0] = 0\n",
    "\n",
    "        svm_train = svm.model.predict(trainC[0])\n",
    "        svm_train[svm_train < 0] = 0\n",
    "        svm_val = svm.model.predict(valC[0])\n",
    "        svm_val[svm_val < 0] = 0\n",
    "\n",
    "        lstm_train = lstm.model.predict(trainL[0])\n",
    "        lstm_train[lstm_train < 0] = 0\n",
    "        lstm_val = lstm.model.predict(valL[0])\n",
    "        lstm_val[lstm_val < 0] = 0\n",
    "\n",
    "        train_pol0, train_pol1, train_pol2, val_pol0, val_pol1, val_pol2 = interpolation(trainL, valL)\n",
    "\n",
    "        x_trainE = np.concatenate((catBoost_train, svm_train, lstm_train, train_pol0, train_pol1, train_pol2), axis=-1)\n",
    "        y_trainE = trainingL[1]\n",
    "        x_valE = np.concatenate((catBoost_val, svm_val, lstm_val, val_pol0, val_pol1, val_pol2), axis=-1)\n",
    "        y_valE = validationL[1]\n",
    "\n",
    "        preds_tra = ens.model.predict(x_trainE)\n",
    "        preds_tra[preds_tra < 0] = 0\n",
    "        preds_val = ens.model.predict(x_valE)\n",
    "        preds_val[preds_val < 0] = 0\n",
    "\n",
    "        datelist = pd.date_range('01-01-2001', end='01-01-2020', freq='YS')\n",
    "        dates = datelist.strftime(\"%Y\").tolist()\n",
    "        \n",
    "        # Total timesteps (nb of months inside this time window) = 228\n",
    "        month_datelist = pd.date_range('01-01-2001', end='31-12-2019', freq='MS')\n",
    "        ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "        res = []\n",
    "        res.append('Department,NRMSE 0-19 Training,NRMSE All Training,NRMSE 0-19 Validation,NRMSE All Validation')\n",
    "\n",
    "        for i in range(nb_deps):\n",
    "            fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18,9))\n",
    "\n",
    "            idxt = preds_tra.shape[0]//nb_deps\n",
    "            idxv = preds_val.shape[0]//nb_deps        \n",
    "\n",
    "            t1 = ts[12:12+idxt]\n",
    "            t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "            # Reverting Data Norm ---------\n",
    "            pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "            pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "            gtt = scaler.inverse_transform(trainL[1][idxt*i:idxt*(i+1), :])\n",
    "            gtv = scaler.inverse_transform(valL[1][idxv*i:idxv*(i+1), :])\n",
    "            # ------------------------------\n",
    "\n",
    "            baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "            base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "            axes[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "            axes[0].plot(t1, pt[:,0], '--', color='red', label = 'Ensemble prediction on Training data')\n",
    "            axes[0].plot(t2, pv[:,0], '-', color='red', label = 'Ensemble prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            axes[1].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "            axes[1].plot(t1, pt[:,1], '--', color='red', label = 'Ensemble prediction on Training data')\n",
    "            axes[1].plot(t2, pv[:,1], '-', color='red', label = 'Ensemble prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[1].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[1].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            # Configure visualization\n",
    "            axes[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left')\n",
    "            axes[0].set(ylabel='DIR Total Population')\n",
    "            axes[1].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "            axes[1].legend(ncol=3, loc='lower center', bbox_to_anchor=(0.5,-0.5), shadow=True)\n",
    "            axes[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "            axes[1].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "            for ax in axes:\n",
    "                ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "                ax.grid(True, linewidth=0.5)\n",
    "                ax.set_xlim(ts[0],ts[-1]+5)\n",
    "            axes[0].set_xticklabels([])\n",
    "            axes[1].set_xticklabels(dates)\n",
    "\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            print('--------------------- TRAINING scores ---------------------')\n",
    "            rmse1, r2 = compute_scores(gtt[:,0], pt[:,0])\n",
    "            print('N-RMSE (group total): ', rmse1)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse2, r2 = compute_scores(gtt[:,1], pt[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse2)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            print('-------------------- VALIDATION scores ---------------------')\n",
    "            rmse3, r2 = compute_scores(gtv[:,0], pv[:,0])\n",
    "            print('N-RMSE (group total): ', rmse3)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse4, r2 = compute_scores(gtv[:,1], pv[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse4)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            res.append('{},{},{},{},{}'.format(DEP_NAMES[i], rmse2, rmse1, rmse4, rmse3))\n",
    "            print('\\n##########################################################################################################')\n",
    "\n",
    "        # Save results\n",
    "        today = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
    "        with open(join(config['metrics'], \"Brazil\", 'rf_ensemble-'+today+'.csv'),'w') as f:\n",
    "            print('Writing performance metrics to file...')\n",
    "            for item in res:\n",
    "                f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FAj0jXtZy1sc",
   "metadata": {
    "id": "FAj0jXtZy1sc"
   },
   "source": [
    "Compute confidence intervals for the ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j2Ym-LbAVnvH",
   "metadata": {
    "id": "j2Ym-LbAVnvH"
   },
   "outputs": [],
   "source": [
    "# Load the inner models\n",
    "cat_boost.load(glob(join(config['output'], \"Brazil\", \"catboost*\"))[0])\n",
    "svm.load(glob(join(config['output'], \"Brazil\", \"svm*\"))[0])\n",
    "lstm.load(glob(join(config['output'], \"Brazil\", \"lstm*\"))[0])\n",
    "\n",
    "# Ensemble\n",
    "ens.load(glob(join(config['output'], \"Brazil\", \"rf*\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fpt-M0gl7SbK",
   "metadata": {
    "id": "fpt-M0gl7SbK"
   },
   "outputs": [],
   "source": [
    "trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "trainL, valL = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "catBoost_train = cat_boost.model.predict(trainC[0])\n",
    "catBoost_train[catBoost_train < 0] = 0\n",
    "catBoost_val = cat_boost.model.predict(valC[0])\n",
    "catBoost_val[catBoost_val < 0] = 0\n",
    "\n",
    "svm_train = svm.model.predict(trainC[0])\n",
    "svm_train[svm_train < 0] = 0\n",
    "svm_val = svm.model.predict(valC[0])\n",
    "svm_val[svm_val < 0] = 0\n",
    "\n",
    "lstm_train = lstm.model.predict(trainL[0])\n",
    "lstm_train[lstm_train < 0] = 0\n",
    "lstm_val = lstm.model.predict(valL[0])\n",
    "lstm_val[lstm_val < 0] = 0\n",
    "\n",
    "train_pol0, train_pol1, train_pol2, val_pol0, val_pol1, val_pol2 = interpolation(trainL, valL)\n",
    "\n",
    "x_trainE = np.concatenate((catBoost_train, svm_train, lstm_train, train_pol0, train_pol1, train_pol2), axis = -1)\n",
    "y_trainE = trainingL[1]\n",
    "x_valE = np.concatenate((catBoost_val, svm_val, lstm_val, val_pol0, val_pol1, val_pol2), axis = -1)\n",
    "y_valE = validationL[1]\n",
    "\n",
    "print(x_trainE.shape, y_trainE.shape, x_valE.shape, y_valE.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5F4bagE1W-ys",
   "metadata": {
    "id": "5F4bagE1W-ys"
   },
   "outputs": [],
   "source": [
    "def pred_ints(model, X, percentile=95):\n",
    "    err_down = []\n",
    "    err_up = []\n",
    "    for x in range(len(X)):\n",
    "        preds = []\n",
    "        for pred in model.estimators_:\n",
    "            preds.append(pred.predict(X[x].reshape(1, -1))[0])\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        err_down.append(np.percentile(preds, (100 - percentile) / 2., axis=0))\n",
    "        err_up.append(np.percentile(preds, 100 - (100 - percentile) / 2., axis=0))\n",
    "    return np.array(err_down), np.array(err_up)\n",
    "\n",
    "train_err_down, train_err_up = pred_ints(ens.model, x_trainE, percentile=95)\n",
    "val_err_down, val_err_up = pred_ints(ens.model, x_valE, percentile=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IaSxa8z-4QYt",
   "metadata": {
    "id": "IaSxa8z-4QYt"
   },
   "outputs": [],
   "source": [
    "preds_tra = ens.model.predict(x_trainE)\n",
    "preds_tra[preds_tra < 0] = 0\n",
    "preds_val = ens.model.predict(x_valE)\n",
    "preds_val[preds_val < 0] = 0\n",
    "\n",
    "datelist = pd.date_range('01-01-2001', end='01-01-2020', freq='YS')\n",
    "dates = datelist.strftime(\"%Y\").tolist()\n",
    "\n",
    "# Total timesteps (nb of months inside this time window) = 228\n",
    "month_datelist = pd.date_range('01-01-2001', end='31-12-2019', freq='MS')\n",
    "ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "for i in range(nb_deps):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18,9))\n",
    "\n",
    "    idxt = preds_tra.shape[0]//nb_deps\n",
    "    idxv = preds_val.shape[0]//nb_deps\n",
    "\n",
    "    t1 = ts[12:12+idxt]\n",
    "    t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "    # Reverting Data Norm ---------\n",
    "    pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "    pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    gtt = scaler.inverse_transform(trainL[1][idxt*i:idxt*(i+1), :])\n",
    "    gtv = scaler.inverse_transform(valL[1][idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    lstm_pt = scaler.inverse_transform(lstm_train[idxt*i:idxt*(i+1), :])\n",
    "    lstm_pv = scaler.inverse_transform(lstm_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    svm_pt = scaler.inverse_transform(svm_train[idxt*i:idxt*(i+1), :])\n",
    "    svm_pv = scaler.inverse_transform(svm_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    cb_pt = scaler.inverse_transform(catBoost_train[idxt*i:idxt*(i+1), :])\n",
    "    cb_pv = scaler.inverse_transform(catBoost_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    t_lci = scaler.inverse_transform(train_err_down[idxt*i:idxt*(i+1), :])\n",
    "    t_uci = scaler.inverse_transform(train_err_up[idxt*i:idxt*(i+1), :])\n",
    "\n",
    "    v_lci = scaler.inverse_transform(val_err_down[idxv*i:idxv*(i+1), :])\n",
    "    v_uci = scaler.inverse_transform(val_err_up[idxv*i:idxv*(i+1), :])\n",
    "    # ------------------------------\n",
    "    \n",
    "    baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "    base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "    ########################\n",
    "    ### Total population ###\n",
    "    ########################\n",
    "    axes[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "    axes[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "    axes[0].plot(t1, pt[:,0], '--', color='red', label = 'Ensemble prediction on Training data')\n",
    "    axes[0].fill_between(t1, y1=(pt[:,0]-t_lci[:,0]), y2=(pt[:,0]+t_uci[:,0]), facecolor='red', alpha=.1, label='Ensemble CI')\n",
    "    axes[0].plot(t2, pv[:,0], '-', color='red', label = 'Ensemble prediction on Validation data')\n",
    "    axes[0].fill_between(t2, y1=(pv[:,0]-v_lci[:,0]), y2=(pv[:,0]+v_uci[:,0]), facecolor='red', alpha=.1)\n",
    "\n",
    "    # Baseline model\n",
    "    axes[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "    axes[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model CI')\n",
    "    axes[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "    #####################\n",
    "    ### Children 0-19 ###\n",
    "    #####################\n",
    "    axes[1].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "    axes[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "    axes[1].plot(t1, pt[:,1], '--', color='red', label = 'Ensemble prediction on Training data')\n",
    "    axes[1].fill_between(t1, y1=(pt[:,1]-t_lci[:,1]), y2=(pt[:,1]+t_uci[:,1]), facecolor='red', alpha=.1, label='Ensemble CI')\n",
    "    axes[1].plot(t2, pv[:,1], '-', color='red', label = 'Ensemble prediction on Validation data')\n",
    "    axes[1].fill_between(t2, y1=(pv[:,1]-v_lci[:,1]), y2=(pv[:,1]+v_uci[:,1]), facecolor='red', alpha=.1)\n",
    "\n",
    "    # Baseline model\n",
    "    axes[1].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "    axes[1].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model CI')\n",
    "    axes[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "    # Configure visualization\n",
    "    axes[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left')\n",
    "    axes[0].set(ylabel='DIR Total Population')\n",
    "    axes[1].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "    axes[1].legend(ncol=3, loc='lower center', bbox_to_anchor=(0.5,-0.5), shadow=True)\n",
    "    axes[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "    axes[1].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "        ax.grid(True, linewidth=0.5)\n",
    "        ax.set_xlim(ts[0],ts[-1]+5)\n",
    "    axes[0].set_xticklabels([])\n",
    "    axes[1].set_xticklabels(dates)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print('\\n##########################################################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WA7JBRiVyLxC",
   "metadata": {
    "id": "WA7JBRiVyLxC"
   },
   "source": [
    "**Get time series plots with validation period zoomed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cKbtZp7hTY-",
   "metadata": {
    "id": "4cKbtZp7hTY-"
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "y_rescaled = False\n",
    "\n",
    "preds_tra = ens.model.predict(x_trainE)\n",
    "preds_tra[preds_tra < 0] = 0\n",
    "preds_val = ens.model.predict(x_valE)\n",
    "preds_val[preds_val < 0] = 0\n",
    "\n",
    "datelist = pd.date_range('01-01-2001', end='01-01-2020', freq='YS')\n",
    "dates = datelist.strftime(\"%Y\").tolist()\n",
    "\n",
    "# Total timesteps (nb of months inside this time window) = 228\n",
    "month_datelist = pd.date_range('01-01-2001', end='31-12-2019', freq='MS')\n",
    "ts = np.arange(0, len(month_datelist), 1)\n",
    "val_dates = pd.date_range('01-01-2017', end='31-12-2019', freq = 'M').strftime(\"%m-%Y\").tolist()\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(nb_deps):\n",
    "    fig, (axes1, axes2) = plt.subplots(nrows=2, ncols=2, figsize=(25,9), gridspec_kw={'width_ratios': [7, 3], 'wspace':0.01, 'hspace':0.06})\n",
    "\n",
    "    idxt = preds_tra.shape[0]//nb_deps\n",
    "    idxv = preds_val.shape[0]//nb_deps        \n",
    "\n",
    "    t1 = ts[12:12+idxt]\n",
    "    t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "    # Reverting Data Norm ---------\n",
    "    pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "    pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    gtt = scaler.inverse_transform(trainL[1][idxt*i:idxt*(i+1), :])\n",
    "    gtv = scaler.inverse_transform(valL[1][idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    t_lci = scaler.inverse_transform(train_err_down[idxt*i:idxt*(i+1), :])\n",
    "    t_uci = scaler.inverse_transform(train_err_up[idxt*i:idxt*(i+1), :])\n",
    "\n",
    "    v_lci = scaler.inverse_transform(val_err_down[idxv*i:idxv*(i+1), :])\n",
    "    v_uci = scaler.inverse_transform(val_err_up[idxv*i:idxv*(i+1), :])\n",
    "    # ------------------------------\n",
    "    \n",
    "    baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "    base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "    #________________________________________________________________________\n",
    "    # TOTAL POPULATION\n",
    "    axes1[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "    axes1[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "    axes1[0].plot(t1, pt[:,0], '-', color='red', label = 'Ensemble prediction')\n",
    "    axes1[0].fill_between(t1, y1=(pt[:,0]-t_lci[:,0]), y2=(pt[:,0]+t_uci[:,0]), facecolor='red', alpha=.1, label='Ensemble CI')\n",
    "    axes1[0].plot(t2, pv[:,0], '-', color='red', label = 'Ensemble prediction')\n",
    "    axes1[0].fill_between(t2, y1=(pv[:,0]-v_lci[:,0]), y2=(pv[:,0]+v_uci[:,0]), facecolor='red', alpha=.1)\n",
    "\n",
    "    # Baseline model\n",
    "    axes1[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "    axes1[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model CI')\n",
    "    axes1[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes1[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "    # Validation Zoom\n",
    "    axes1[1].plot(t2, gtv[:,0], '-', color='orange')\n",
    "    axes1[1].plot(t2, pv[:,0], '-', color='red')\n",
    "    axes1[1].fill_between(t2, y1=(pv[:,0]-v_lci[:,0]), y2=(pv[:,0]+v_uci[:,0]), facecolor='red', alpha=.1)\n",
    "    axes1[1].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes1[1].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "    #________________________________________________________________________\n",
    "    # AGE 0-19\n",
    "    axes2[0].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "    axes2[0].plot(t2, gtv[:,1], '-', color='orange')\n",
    "    axes2[0].plot(t1, pt[:,1], '-', color='red', label = 'Ensemble prediction')\n",
    "    axes2[0].fill_between(t1, y1=(pt[:,1]-t_lci[:,1]), y2=(pt[:,1]+t_uci[:,1]), facecolor='red', alpha=.1, label='Ensemble CI')\n",
    "    axes2[0].plot(t2, pv[:,1], '-', color='red', label = 'Ensemble prediction')\n",
    "    axes2[0].fill_between(t2, y1=(pv[:,1]-v_lci[:,1]), y2=(pv[:,1]+v_uci[:,1]), facecolor='red', alpha=.1)\n",
    "\n",
    "    # Baseline model\n",
    "    axes2[0].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "    axes2[0].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model CI')\n",
    "    axes2[0].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes2[0].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "    # Validation Zoom\n",
    "    axes2[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "    axes2[1].plot(t2, pv[:,1], '-', color='red')\n",
    "    axes2[1].fill_between(t2, y1=(pv[:,1]-v_lci[:,1]), y2=(pv[:,1]+v_uci[:,1]), facecolor='red', alpha=.1)\n",
    "    axes2[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes2[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "    # Configure visualization\n",
    "    axes1[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left', fontweight=\"bold\")\n",
    "    axes1[1].set_title('Validation period', fontsize = 15)\n",
    "    axes1[0].set(ylabel='DIR Total Population')\n",
    "    axes2[0].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "    axes2[1].set(xlabel='Month-Year')\n",
    "    axes1[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "    axes2[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color='orange', lw=2),\n",
    "                       (Line2D([0], [0], color='red', lw=2), Patch(facecolor='red', alpha=.1, lw=2)),\n",
    "                       (Line2D([0], [0], color='forestgreen', lw=2), Patch(facecolor='forestgreen', alpha=.1, lw=2))                             \n",
    "                    ]\n",
    "    axes2[0].legend(handles=legend_elements, labels=['Observed Cases', 'Ensemble Prediction + CI', 'Baseline Prediction + CI'], \n",
    "                    ncol=3, loc='lower center', bbox_to_anchor=(0.71,-0.55), shadow=True)\n",
    "\n",
    "\n",
    "    for ax in [axes1[0], axes2[0]]:\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "        ax.grid(True, linewidth=0.5)\n",
    "        ax.set_xlim(ts[0],ts[-1]+5)\n",
    "\n",
    "    for ax in [axes1[1], axes2[1]]:\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_xticks(np.arange(t2[0],t2[0]+len(t2),3))\n",
    "        ax.yaxis.tick_right()\n",
    "        ax.grid(True, linewidth=0.5)\n",
    "        ax.set_xlim(t2[0]-1,)\n",
    "\n",
    "    if y_rescaled:\n",
    "        ymax = max(np.max(gtt[:,0]), np.max(gtv[:,0]), np.max(pt[:,0]), np.max(pv[:,0]))\n",
    "        ymax_019 = max(np.max(gtt[:,1]), np.max(gtv[:,1]), np.max(pt[:,1]), np.max(pv[:,1]))\n",
    "        ymin = min(np.min(gtt[:,0]), np.min(gtv[:,0]), np.min(pt[:,0]), np.min(pv[:,0]))\n",
    "        ymin_019 = min(np.min(gtt[:,1]), np.min(gtv[:,1]), np.min(pt[:,1]), np.min(pv[:,1]))\n",
    "\n",
    "        val_ymax = max(np.max(gtv[:,0]), np.max(pv[:,0]))\n",
    "        val_ymax_019 = max(np.max(gtv[:,1]), np.max(pv[:,1]))\n",
    "        val_ymin = min(np.min(gtv[:,0]), np.min(pv[:,0]))\n",
    "        val_ymin_019 = min(np.min(gtv[:,1]), np.min(pv[:,1]))\n",
    "\n",
    "        # 5% bottom margin and 10% top margin\n",
    "        minY = ymin-0.05*(ymax-ymin)\n",
    "        maxY = ymax+0.1*(ymax-ymin)\n",
    "        minY_019 = ymin_019-0.05*(ymax_019-ymin_019)\n",
    "        maxY_019 = ymax_019+0.1*(ymax_019-ymin_019)\n",
    "        val_minY = val_ymin-0.05*(val_ymax-val_ymin)\n",
    "        val_maxY = val_ymax+0.1*(val_ymax-val_ymin)\n",
    "        val_minY_019 = val_ymin_019-0.05*(val_ymax_019-val_ymin_019)\n",
    "        val_maxY_019 = val_ymax_019+0.1*(val_ymax_019-val_ymin_019)\n",
    "\n",
    "        axes1[0].set_ylim((minY, maxY))\n",
    "        axes2[0].set_ylim((minY_019, maxY_019))\n",
    "        axes1[1].set_ylim((val_minY, val_maxY))\n",
    "        axes2[1].set_ylim((val_minY_019, val_maxY_019))\n",
    "\n",
    "    axes1[0].set_xticklabels([])\n",
    "    axes2[0].set_xticklabels(dates)\n",
    "    axes1[1].set_xticklabels([])\n",
    "    axes2[1].set_xticklabels(val_dates[::3], rotation=45)  \n",
    "\n",
    "    plt.show() \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # print('-------------------- VALIDATION scores ---------------------')\n",
    "    rmseAll, _ = compute_scores(gtv[:,0], pv[:,0])\n",
    "    rmseAll_uci, _ = compute_scores(gtv[:,0], v_uci[:,0])\n",
    "    rmseAll_lci, _ = compute_scores(gtv[:,0], v_lci[:,0])\n",
    "\n",
    "    rmse019, _ = compute_scores(gtv[:,1], pv[:,1])\n",
    "    rmse019_uci, _ = compute_scores(gtv[:,1], v_uci[:,1])\n",
    "    rmse019_lci, _ = compute_scores(gtv[:,1], v_lci[:,1])\n",
    "\n",
    "    data.append([i, DEP_NAMES[i], rmseAll, rmseAll_uci, rmseAll_lci, rmse019, rmse019_uci, rmse019_lci])\n",
    "\n",
    "    print('\\n##########################################################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0y2iVphNi2Bo",
   "metadata": {
    "id": "0y2iVphNi2Bo"
   },
   "source": [
    "# Compare Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9o0QM6JIGJaC",
   "metadata": {
    "id": "9o0QM6JIGJaC"
   },
   "source": [
    "## Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mAjoscoBGJGf",
   "metadata": {
    "id": "mAjoscoBGJGf"
   },
   "outputs": [],
   "source": [
    "trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "trainL, valL = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "catBoost_train = cat_boost.model.predict(trainC[0])\n",
    "catBoost_train[catBoost_train < 0] = 0\n",
    "catBoost_val = cat_boost.model.predict(valC[0])\n",
    "catBoost_val[catBoost_val < 0] = 0\n",
    "\n",
    "svm_train = svm.model.predict(trainC[0])\n",
    "svm_train[svm_train < 0] = 0\n",
    "svm_val = svm.model.predict(valC[0])\n",
    "svm_val[svm_val < 0] = 0\n",
    "\n",
    "lstm_train = lstm.model.predict(trainL[0])\n",
    "lstm_train[lstm_train < 0] = 0\n",
    "lstm_val = lstm.model.predict(valL[0])\n",
    "lstm_val[lstm_val < 0] = 0\n",
    "\n",
    "train_pol0, train_pol1, train_pol2, val_pol0, val_pol1, val_pol2 = interpolation(trainL, valL)\n",
    "\n",
    "x_trainE = np.concatenate((catBoost_train, svm_train, lstm_train, train_pol0, train_pol1, train_pol2), axis = -1)\n",
    "y_trainE = trainingL[1]\n",
    "x_valE = np.concatenate((catBoost_val, svm_val, lstm_val, val_pol0, val_pol1, val_pol2), axis = -1)\n",
    "y_valE = validationL[1]\n",
    "\n",
    "preds_tra = ens.model.predict(x_trainE)\n",
    "preds_tra[preds_tra < 0] = 0\n",
    "preds_val = ens.model.predict(x_valE)\n",
    "preds_val[preds_val < 0] = 0\n",
    "\n",
    "dates = pd.date_range('01-01-2001', end='31-12-2019', freq = 'Y').strftime(\"%Y\").tolist()\n",
    "val_dates = pd.date_range('01-01-2016', end='31-12-2019', freq = 'M').strftime(\"%m-%Y\").tolist()\n",
    "\n",
    "for i in range(nb_deps):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(30,10), gridspec_kw={'width_ratios': [3, 2]})\n",
    "    t1 = np.arange(0, preds_tra.shape[0]//nb_deps, 1)\n",
    "    t2 = preds_tra.shape[0]//nb_deps + np.arange(0, preds_val.shape[0]//nb_deps, 1)\n",
    "\n",
    "    idxt = preds_tra.shape[0]//nb_deps\n",
    "    idxv = preds_val.shape[0]//nb_deps\n",
    "\n",
    "    # Reverting Data Norm ---------\n",
    "    lstm_pt = scaler.inverse_transform(lstm_train[idxt*i:idxt*(i+1), :])\n",
    "    lstm_pv = scaler.inverse_transform(lstm_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    ens_pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "    ens_pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    svm_pt = scaler.inverse_transform(svm_train[idxt*i:idxt*(i+1), :])\n",
    "    svm_pv = scaler.inverse_transform(svm_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    cb_pt = scaler.inverse_transform(catBoost_train[idxt*i:idxt*(i+1), :])\n",
    "    cb_pv = scaler.inverse_transform(catBoost_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    gtt = scaler.inverse_transform(trainL[1][idxt*i:idxt*(i+1), :])\n",
    "    gtv = scaler.inverse_transform(valL[1][idxv*i:idxv*(i+1), :])\n",
    "    # ------------------------------\n",
    "    \n",
    "    ##############################\n",
    "    ### Cases Total Population ###\n",
    "    ##############################\n",
    "    ax1.plot(t1, gtt[:,0], '-', color='orange', label='Observed Cases')\n",
    "    ax1.plot(t2, gtv[:,0], '-', color='orange')\n",
    "    ax1.set_title('Dengue Incidence Rate (DIR) total for {}'.format(DEP_NAMES[i]), fontsize = 18)\n",
    "\n",
    "    # Ensemble\n",
    "    ax1.plot(t1, ens_pt[:,0], '--', color='darkgrey', label='Ensemble on Training data')\n",
    "    ax1.plot(t2, ens_pv[:,0], '-', color='black', label='Ensemble on Validation data')\n",
    "    # LSTM\n",
    "    ax1.plot(t1, lstm_pt[:,0], '--', color='plum', label='LSTM on Training data')\n",
    "    ax1.plot(t2, lstm_pv[:,0], '-', color='purple', label='LSTM on Validation data')\n",
    "    # SVM\n",
    "    ax1.plot(t1, svm_pt[:,0], '--', color='dodgerblue', label='SVM on Training data')\n",
    "    ax1.plot(t2, svm_pv[:,0], '-', color='blue', label='SVM on Validation data')\n",
    "    # CatBoost\n",
    "    ax1.plot(t1, cb_pt[:,0], '--', color='yellowgreen', label='CatBoost on Training data')\n",
    "    ax1.plot(t2, cb_pv[:,0], '-', color='green', label='CatBoost on Validation data')\n",
    "    ax1.set_xticks(np.arange(0, idxt+idxv, 11))\n",
    "    ax1.set_xticklabels(dates)\n",
    "    ax1.set_xlim(t1[0],)   \n",
    "    ax1.set(xlabel='Year', ylabel='DIR')\n",
    "    ax1.grid(True, linewidth=0.5)\n",
    "\n",
    "    #################\n",
    "    ax2.plot(t2, gtv[:,0], '-', color='orange', label='Observed Cases')\n",
    "    ax2.set_title('Zoom on the Validation period', fontsize = 18)\n",
    "    ax2.plot(t2, ens_pv[:,0], '-', color='black', label='Ensemble')\n",
    "    ax2.plot(t2, lstm_pv[:,0], '-', color='purple', label='LSTM')\n",
    "    ax2.plot(t2, svm_pv[:,0], '-', color='blue', label='SVM')\n",
    "    ax2.plot(t2, cb_pv[:,0], '-', color='green', label='CatBoost')\n",
    "    ax2.set_xticks(t2)\n",
    "    ax2.set_xticklabels(val_dates, rotation=45)\n",
    "    ax2.set_xlim(t2[0],)   \n",
    "    ax2.set(xlabel='Month-Year', ylabel='DIR')\n",
    "    ax2.grid(True, linewidth=0.5)\n",
    "\n",
    "    #############################\n",
    "    ### Cases Population 0-19 ###\n",
    "    #############################\n",
    "    ax3.plot(t1, gtt[:,1], '-', color='orange', label='Observed Cases')\n",
    "    ax3.plot(t2, gtv[:,1], '-', color='orange')\n",
    "    ax3.set_title('Dengue Incidence Rate (DIR) 0-19 for {}'.format(DEP_NAMES[i]), fontsize = 18)\n",
    "\n",
    "    # Ensemble\n",
    "    ax3.plot(t1, ens_pt[:,1], '--', color='darkgrey', label='Ensemble on Training data')\n",
    "    ax3.plot(t2, ens_pv[:,1], '-', color='black', label='Ensemble on Validation data')\n",
    "    # LSTM\n",
    "    ax3.plot(t1, lstm_pt[:,1], '--', color='plum', label='LSTM on Training data')\n",
    "    ax3.plot(t2, lstm_pv[:,1], '-', color='purple', label='LSTM on Validation data')\n",
    "    # SVM\n",
    "    ax3.plot(t1, svm_pt[:,1], '--', color='dodgerblue', label='SVM on Training data')\n",
    "    ax3.plot(t2, svm_pv[:,1], '-', color='blue', label='SVM on Validation data')\n",
    "    # CatBoost\n",
    "    ax3.plot(t1, cb_pt[:,1], '--', color='yellowgreen', label='CatBoost on Training data')\n",
    "    ax3.plot(t2, cb_pv[:,1], '-', color='green', label='CatBoost on Validation data')\n",
    "    ax3.set_xticks(np.arange(0, idxt+idxv, 11))\n",
    "    ax3.set_xticklabels(dates)\n",
    "    ax3.set_xlim(t1[0],)\n",
    "    ax3.set(xlabel='Year', ylabel='DIR')\n",
    "    ax3.grid(True, linewidth=0.5) \n",
    "\n",
    "    #################\n",
    "    ax4.plot(t2, gtv[:,1], '-', color='orange', label='Observed Cases')\n",
    "    ax4.set_title('Zoom on the Validation period', fontsize = 18)\n",
    "    ax4.plot(t2, ens_pv[:,1], '-', color='black', label='Ensemble')\n",
    "    ax4.plot(t2, lstm_pv[:,1], '-', color='purple', label='LSTM')\n",
    "    ax4.plot(t2, svm_pv[:,1], '-', color='blue', label='LSTM')\n",
    "    ax4.plot(t2, cb_pv[:,1], '-', color='green', label='CatBoost')\n",
    "    ax4.set_xticks(t2)\n",
    "    ax4.set_xticklabels(val_dates, rotation=45)\n",
    "    ax4.set_xlim(t2[0],)\n",
    "    ax4.set(xlabel='Month-Year', ylabel='DIR')\n",
    "    ax4.grid(True, linewidth=0.5)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    ax3.legend(fontsize=14, ncol=4, shadow=True, loc='center', bbox_to_anchor=(0.7,-0.5))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print('\\n##########################################################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XZ1ow2z-GAZx",
   "metadata": {
    "id": "XZ1ow2z-GAZx"
   },
   "source": [
    "## Normalized RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9qk-84BmVGWb",
   "metadata": {
    "id": "9qk-84BmVGWb"
   },
   "outputs": [],
   "source": [
    "catD = pd.read_csv(glob(join(config['metrics'], \"Brazil\", 'catboost*'))[0])\n",
    "svmD = pd.read_csv(glob(join(config['metrics'], \"Brazil\", 'svm*'))[0])\n",
    "lstmD = pd.read_csv(glob(join(config['metrics'], \"Brazil\", 'lstm*'))[0])\n",
    "rf_ensD = pd.read_csv(glob(join(config['metrics'], \"Brazil\", 'rf*'))[0])\n",
    "baseD = pd.read_csv(glob(join(config['metrics'], \"Brazil\", 'baseline*'))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TyT39eNgaGFd",
   "metadata": {
    "id": "TyT39eNgaGFd"
   },
   "source": [
    "Comparison between all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R6GNhs9ljFdJ",
   "metadata": {
    "id": "R6GNhs9ljFdJ"
   },
   "outputs": [],
   "source": [
    "X = np.arange(0, 2*nb_deps, 2)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))\n",
    "axes[0].bar(X, catD[\"NRMSE 0-19 Training\"], color='red', label='CatBoost', width=0.3)\n",
    "axes[0].bar(X+0.3, svmD[\"NRMSE 0-19 Training\"], color='blue', label='SVM', width=0.3)\n",
    "axes[0].bar(X+0.6, lstmD[\"NRMSE 0-19 Training\"], color='orange', label='LSTM', width=0.3)\n",
    "axes[0].bar(X+0.9, rf_ensD[\"NRMSE 0-19 Training\"], color='purple', label='Ensemble', width=0.3)\n",
    "axes[0].bar(X+1.2, baseD[\"NRMSE 0-19 Training\"], color='green', label='Baseline', width=0.3)\n",
    "axes[0].set_title('N-RMSE 0-19 Training')\n",
    "\n",
    "axes[1].bar(X, catD[\"NRMSE All Training\"], color='red', label='CatBoost', width=0.3)\n",
    "axes[1].bar(X+0.3, svmD[\"NRMSE All Training\"], color='blue', label='SVM', width=0.3)\n",
    "axes[1].bar(X+0.6, lstmD[\"NRMSE All Training\"], color='orange', label='LSTM', width=0.3)\n",
    "axes[1].bar(X+0.9, rf_ensD[\"NRMSE All Training\"], color='purple', label='Ensemble', width=0.3)\n",
    "axes[1].bar(X+1.2, baseD[\"NRMSE All Training\"], color='green', label='Baseline', width=0.3)\n",
    "axes[1].set_title('N-RMSE All Training')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks(0.6+np.arange(0, 2*nb_deps, 2))\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.legend(shadow=True, ncol=5)\n",
    "    ax.set_xlim(X[0]-0.6, X[-1]+2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))\n",
    "axes[0].bar(X, catD[\"NRMSE 0-19 Validation\"], color='red', label='CatBoost', width=0.3)\n",
    "axes[0].bar(X+0.3, svmD[\"NRMSE 0-19 Validation\"], color='blue', label='SVM', width=0.3)\n",
    "axes[0].bar(X+0.6, lstmD[\"NRMSE 0-19 Validation\"], color='orange', label='LSTM', width=0.3)\n",
    "axes[0].bar(X+0.9, rf_ensD[\"NRMSE 0-19 Validation\"], color='purple', label='Ensemble', width=0.3)\n",
    "axes[0].bar(X+1.2, baseD[\"NRMSE 0-19 Validation\"], color='green', label='Baseline', width=0.3)\n",
    "axes[0].set_title('N-RMSE 0-19 Validation')\n",
    "\n",
    "axes[1].bar(X, catD[\"NRMSE All Validation\"], color='red', label='CatBoost', width=0.3)\n",
    "axes[1].bar(X+0.3, svmD[\"NRMSE All Validation\"], color='blue', label='SVM', width=0.3)\n",
    "axes[1].bar(X+0.6, lstmD[\"NRMSE All Validation\"], color='orange', label='LSTM', width=0.3)\n",
    "axes[1].bar(X+0.9, rf_ensD[\"NRMSE All Validation\"], color='purple', label='Ensemble', width=0.3)\n",
    "axes[1].bar(X+1.2, baseD[\"NRMSE All Validation\"], color='green', label='Baseline', width=0.3)\n",
    "axes[1].set_title('N-RMSE All Validation')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks(0.6+np.arange(0, 2*nb_deps, 2))\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.legend(shadow=True, ncol=5)\n",
    "    ax.set_xlim(X[0]-0.6, X[-1]+2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oq4kVB9WaRBH",
   "metadata": {
    "id": "oq4kVB9WaRBH"
   },
   "source": [
    "Comparison between 2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R-5bTep37ma5",
   "metadata": {
    "id": "R-5bTep37ma5"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))\n",
    "axes[0].bar(rf_ensD[\"Department\"], baseD[\"NRMSE 0-19 Training\"] - rf_ensD[\"NRMSE 0-19 Training\"], color='red', label='Ensemble vs. Baseline', width=0.25)\n",
    "axes[0].set_title('N-RMSE comparison for population 0-19 on Training set')\n",
    "axes[1].bar(rf_ensD[\"Department\"], baseD[\"NRMSE All Training\"]- rf_ensD[\"NRMSE All Training\"], color='red', label='Ensemble vs. Baseline', width=0.25)\n",
    "axes[1].set_title('N-RMSE comparison for All population on Training set')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.set_ylabel('NRMSE$_{baseline}$ $-$ NRMSE$_{proposed}$')\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(rf_cnn_ensD_withoutMaxDepth[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))\n",
    "axes[0].bar(rf_ensD[\"Department\"], baseD[\"NRMSE 0-19 Validation\"] - rf_ensD[\"NRMSE 0-19 Validation\"], color='red', label='Ensemble vs. Baseline', width=0.25)\n",
    "axes[0].set_title('N-RMSE comparison for population 0-19 on Validation set')\n",
    "axes[1].bar(rf_ensD[\"Department\"], baseD[\"NRMSE All Validation\"]- rf_ensD[\"NRMSE All Validation\"], color='red', label='Ensemble vs. Baseline', width=0.25)\n",
    "axes[1].set_title('N-RMSE comparison for All population on Validation set')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.set_ylabel('NRMSE$_{baseline}$ $-$ NRMSE$_{proposed}$')\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(rf_ensD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zgdr5w9r_dYm",
   "metadata": {
    "id": "zgdr5w9r_dYm"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(17,8))\n",
    "\n",
    "X = np.arange(len(rf_ensD[\"Department\"].values))\n",
    "\n",
    "axes.bar(X, baseD[\"NRMSE 0-19 Validation\"] - rf_ensD[\"NRMSE 0-19 Validation\"], color='red', label='Population 0-19', width=0.25)\n",
    "axes.bar(X+0.25, baseD[\"NRMSE All Validation\"]- rf_ensD[\"NRMSE All Validation\"], color='blue', label='Population All', width=0.25)\n",
    "\n",
    "axes.set_title('N-RMSE comparison on Validation set', fontsize=18)\n",
    "axes.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "axes.set_ylabel('NRMSE$_{baseline}$ $-$ NRMSE$_{proposed}$', fontsize=16)\n",
    "axes.legend(shadow=True, fontsize=16)\n",
    "axes.grid(True, linewidth=0.5)\n",
    "axes.set_xticks(X+0.125)\n",
    "axes.set_xticklabels(rf_ensD[\"Department\"].values, rotation=90, fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v7BzdPF4oQN6",
   "metadata": {
    "id": "v7BzdPF4oQN6"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))\n",
    "axes[0].bar(catD[\"Department\"], rf_ensD[\"NRMSE 0-19 Training\"] - catD[\"NRMSE 0-19 Training\"], color='red', label='Ensemble vs. CatBoost', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Training')\n",
    "axes[1].bar(catD[\"Department\"], rf_ensD[\"NRMSE All Training\"]- catD[\"NRMSE All Training\"], color='red', label='Ensemble vs. CatBoost', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Training')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))\n",
    "axes[0].bar(catD[\"Department\"], rf_ensD[\"NRMSE 0-19 Validation\"] - catD[\"NRMSE 0-19 Validation\"], color='red', label='Ensemble vs. CatBoost', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Validation')\n",
    "axes[1].bar(catD[\"Department\"], rf_ensD[\"NRMSE All Validation\"]- catD[\"NRMSE All Validation\"], color='red', label='Ensemble vs. CatBoost', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Validation')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oDW2ehUfIeFy",
   "metadata": {
    "id": "oDW2ehUfIeFy"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], rf_ensD[\"NRMSE 0-19 Training\"] - svmD[\"NRMSE 0-19 Training\"], color='red', label='Ensemble RF vs. SVM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Training')\n",
    "axes[1].bar(catD[\"Department\"], rf_ensD[\"NRMSE All Training\"]- svmD[\"NRMSE All Training\"], color='red', label='Ensemble RF vs. SVM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Training')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], rf_ensD[\"NRMSE 0-19 Validation\"] - svmD[\"NRMSE 0-19 Validation\"], color='red', label='Ensemble RF vs. SVM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Validation')\n",
    "axes[1].bar(catD[\"Department\"], rf_ensD[\"NRMSE All Validation\"]- svmD[\"NRMSE All Validation\"], color='red', label='Ensemble RF vs. SVM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Validation')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jqAiaKO_JJ6B",
   "metadata": {
    "id": "jqAiaKO_JJ6B"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], rf_ensD[\"NRMSE 0-19 Training\"] - lstmD[\"NRMSE 0-19 Training\"], color='red', label='Ensemble RF vs. LSTM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Training')\n",
    "axes[1].bar(catD[\"Department\"], rf_ensD[\"NRMSE All Training\"]- lstmD[\"NRMSE All Training\"], color='red', label='Ensemble RF vs. LSTM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Training')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], rf_ensD[\"NRMSE 0-19 Validation\"] - lstmD[\"NRMSE 0-19 Validation\"], color='red', label='Ensemble RF vs. LSTM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Validation')\n",
    "axes[1].bar(catD[\"Department\"], rf_ensD[\"NRMSE All Validation\"] - lstmD[\"NRMSE All Validation\"], color='red', label='Ensemble RF vs. LSTM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Validation')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RWrb-sSaJTCX",
   "metadata": {
    "id": "RWrb-sSaJTCX"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], svmD[\"NRMSE 0-19 Training\"] - lstmD[\"NRMSE 0-19 Training\"], color='red', label='SVM vs. LSTM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Training')\n",
    "axes[1].bar(catD[\"Department\"], svmD[\"NRMSE All Training\"]- lstmD[\"NRMSE All Training\"], color='red', label='SVM vs. LSTM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Training')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], svmD[\"NRMSE 0-19 Validation\"] - lstmD[\"NRMSE 0-19 Validation\"], color='red', label='SVM vs. LSTM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Validation')\n",
    "axes[1].bar(catD[\"Department\"], svmD[\"NRMSE All Validation\"] - lstmD[\"NRMSE All Validation\"], color='red', label='SVM vs. LSTM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Validation')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KUgi7d7eJkm4",
   "metadata": {
    "id": "KUgi7d7eJkm4"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], catD[\"NRMSE 0-19 Training\"] - lstmD[\"NRMSE 0-19 Training\"], color='red', label='CatBoost vs. LSTM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Training')\n",
    "axes[1].bar(catD[\"Department\"], catD[\"NRMSE All Training\"]- lstmD[\"NRMSE All Training\"], color='red', label='CatBoost vs. LSTM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Training')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], catD[\"NRMSE 0-19 Validation\"] - lstmD[\"NRMSE 0-19 Validation\"], color='red', label='CatBoost vs. LSTM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Validation')\n",
    "axes[1].bar(catD[\"Department\"], catD[\"NRMSE All Validation\"] - lstmD[\"NRMSE All Validation\"], color='red', label='CatBoost vs. LSTM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Validation')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xGCFhDDZJu8W",
   "metadata": {
    "id": "xGCFhDDZJu8W"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], catD[\"NRMSE 0-19 Training\"] - svmD[\"NRMSE 0-19 Training\"], color='red', label='CatBoost vs. SVM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Training')\n",
    "axes[1].bar(catD[\"Department\"], catD[\"NRMSE All Training\"]- svmD[\"NRMSE All Training\"], color='red', label='CatBoost vs. SVM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Training')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], catD[\"NRMSE 0-19 Validation\"] - svmD[\"NRMSE 0-19 Validation\"], color='red', label='CatBoost vs. SVM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Validation')\n",
    "axes[1].bar(catD[\"Department\"], catD[\"NRMSE All Validation\"] - svmD[\"NRMSE All Validation\"], color='red', label='CatBoost vs. SVM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Validation')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "7audVqxFBDWh",
    "717e2eae-97ff-475f-bfd5-78e8dcee981d",
    "52ae33ce-0228-4303-a796-ce8a7c50869a",
    "462125c4-fbb2-4fe7-8cfd-3e023fd5356c",
    "c80-UoybcjRp",
    "6a956d2f-f784-4d7e-953d-dfac8c36b3ba",
    "E3iGL37YB9FU",
    "nhxwEojupvjg",
    "0y2iVphNi2Bo",
    "9o0QM6JIGJaC",
    "XZ1ow2z-GAZx"
   ],
   "name": "AI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
