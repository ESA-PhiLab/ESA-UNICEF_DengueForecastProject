{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7audVqxFBDWh",
   "metadata": {
    "id": "7audVqxFBDWh"
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UdxUs05bahYS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27100,
     "status": "ok",
     "timestamp": 1648569465099,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "UdxUs05bahYS",
    "outputId": "0b9ee070-0b1c-4dd5-d150-7e3a01507106"
   },
   "outputs": [],
   "source": [
    "# insert your desired path to work on\n",
    "import os\n",
    "from os.path import join\n",
    "project_path = os.path.dirname(os.getcwd())\n",
    "os.chdir(join('..','data'))\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8tZCHvverUb_",
   "metadata": {
    "id": "8tZCHvverUb_"
   },
   "source": [
    "Run the following section one time per session. This cell links the code folder to the python exectution path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hftunTO1rHKZ",
   "metadata": {
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1648569501394,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "hftunTO1rHKZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(join(project_path, 'code'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33R-Be_BrUuK",
   "metadata": {
    "id": "33R-Be_BrUuK"
   },
   "source": [
    "The following cell allows Jupyter Notebooks to detect changes in external code and to automatically update it without restarting the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d77b30-7bf9-4302-9def-c4f4ea4a5963",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1648569505230,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "12d77b30-7bf9-4302-9def-c4f4ea4a5963"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K2C_kf2otfQF",
   "metadata": {
    "id": "K2C_kf2otfQF"
   },
   "source": [
    "Plots settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "LENlVPDfteud",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1648569534807,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "LENlVPDfteud"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'family':'Arial', 'size':'15', 'weight':'normal'}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44fdd8f-8d3e-46cf-a2d1-d9bde2def03a",
   "metadata": {},
   "source": [
    "Set folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96391720-f4b7-4e3f-bf6f-2cb4b4a01450",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'main_brazil': 'Brazil',\n",
    "    'main_peru': 'Peru',\n",
    "    'baseline': join(project_path, \"baseline_models\"),\n",
    "    'output': join(project_path, \"code\", \"saved_models\"),\n",
    "    'metrics': join(project_path, \"code\", \"metrics\")\n",
    "}\n",
    "\n",
    "# List comprehension for the folder structure code\n",
    "[os.makedirs(val, exist_ok=True) for key, val in config.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d1eed-54e9-4f76-b8f8-42587caaabaa",
   "metadata": {
    "id": "8d3d1eed-54e9-4f76-b8f8-42587caaabaa",
    "tags": []
   },
   "source": [
    "# **AI4Dengue forecasting**\n",
    "![](https://drive.google.com/uc?export=view&id=1J5Bt5Cks-e2IV-dEJLHJkuwXFJNFAZgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "272e0945-3bd3-4660-87bc-ebc9438535fc",
   "metadata": {
    "executionInfo": {
     "elapsed": 3336,
     "status": "ok",
     "timestamp": 1648569579925,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "272e0945-3bd3-4660-87bc-ebc9438535fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from configPeru import DEP_NAMES, GROUPED_VARS, DATA_REDUCER_SETTINGS, DATA_PROCESSING_SETTINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Mmz-HTHisxH3",
   "metadata": {
    "id": "Mmz-HTHisxH3"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e2eae-97ff-475f-bfd5-78e8dcee981d",
   "metadata": {
    "id": "717e2eae-97ff-475f-bfd5-78e8dcee981d"
   },
   "source": [
    "## Load the dataframe\n",
    "**This dataframe comprises all the variables (climatic, epidemiological etc.) acquired for each Department during a defined number of years.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5208ca-5c08-4b12-b4bf-37a49f338d25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 1271,
     "status": "ok",
     "timestamp": 1648569584119,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "8e5208ca-5c08-4b12-b4bf-37a49f338d25",
    "outputId": "7931f92a-632d-451f-f9a6-a86c56f61022"
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(join(config['main_Peru'], \"Peru_Departments_dengue_monthly.csv\"))\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4233a8a-bd01-4edd-96a9-6f8df9733886",
   "metadata": {
    "id": "a4233a8a-bd01-4edd-96a9-6f8df9733886"
   },
   "source": [
    "**'Clean' the dataset (e.g. remove NaN values)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b042e6-c358-4688-b897-e395e312ddc1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1648569584119,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "70b042e6-c358-4688-b897-e395e312ddc1",
    "outputId": "a73ae8fd-0f38-4a8d-dcfb-1d6a4528660f"
   },
   "outputs": [],
   "source": [
    "dataframe.rename(columns={'PopTotal':'PopTotal_UF'}, inplace=True)\n",
    "dataframe.rename(columns={'Pop0_19':'Pop0_19_UF'}, inplace=True)\n",
    "dataframe.rename(columns={'PopTotal_Urban':'PopTotal_Urban_UF'}, inplace=True)\n",
    "dataframe.rename(columns={'PopTotal_Rural':'PopTotal_Rural_UF'}, inplace=True)\n",
    "dataframe['Pop0_19_Urban_UF'] = dataframe['PopTotal_Urban_UF']\n",
    "dataframe['Pop0_19_Rural_UF'] = dataframe['PopTotal_Rural_UF']\n",
    "\n",
    "deps = pd.unique(dataframe['ADM1_PCODE'])\n",
    "deps_dictionary = { i : dep for dep,i in enumerate(deps) }\n",
    "dataframe['ADM1_PCODE'] = dataframe['ADM1_PCODE'].map(deps_dictionary) \n",
    "\n",
    "dataframe = utils.clean(dataframe)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AgWy_v6LJkhM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1648569584119,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "AgWy_v6LJkhM",
    "outputId": "f4e8c1c4-0438-4cf8-8352-987f9c9628c0"
   },
   "outputs": [],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ca977-edc7-4d92-9ddb-47c7bd6fbaca",
   "metadata": {
    "id": "ea3ca977-edc7-4d92-9ddb-47c7bd6fbaca"
   },
   "source": [
    "## Apply Data Reduction\n",
    "**Data reduction is applied to three macro groups in order to reduce the number of variables on which the AI framework will be trained. The variables belonging to each group are set with the *PCAgroups* dictionary. The groups are:**\n",
    "1. ***CLIMATIC VARIABLES***,\n",
    "2. ***GEO VARIABLES***,\n",
    "3. ***SOCIO VARIABLES***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953d325-ae5a-4297-adce-b67f5462bb47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1648569584634,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "a953d325-ae5a-4297-adce-b67f5462bb47",
    "outputId": "515c69da-da19-414a-d587-4ab7d17477c1"
   },
   "outputs": [],
   "source": [
    "print('\\033[1m PCA Excluded Variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['EXCLUDED'])\n",
    "\n",
    "print('\\033[1m Climatic variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['CLIMATIC VARIABLES'])\n",
    "\n",
    "print('\\033[1m Geo variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['GEO VARIABLES'])\n",
    "\n",
    "print('\\033[1m Socio variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['SOCIO VARIABLES'])\n",
    "\n",
    "print('\\033[1m Additional variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['AUXILIAR'])\n",
    "\n",
    "print('\\033[1m Dengue variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['DENGUE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7733c78-84dd-4f71-8b6d-1788c4ea218c",
   "metadata": {
    "id": "b7733c78-84dd-4f71-8b6d-1788c4ea218c"
   },
   "source": [
    "**We selected two types of data reduction methods: PCA (Principal Component Analysis) and PLS (Principal Least Square). The second one is the default solution because it reduces the input data by considering also a second variable that in our case is the Dengue Incidence Rates.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed07eec8-4360-4a04-8a1a-9f47e8aadf00",
   "metadata": {
    "executionInfo": {
     "elapsed": 959,
     "status": "ok",
     "timestamp": 1648569585588,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "ed07eec8-4360-4a04-8a1a-9f47e8aadf00"
   },
   "outputs": [],
   "source": [
    "from data_reduction import pca_reducer, pls_reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493bea11-18e6-49ff-b3db-d749f0812ab9",
   "metadata": {
    "id": "493bea11-18e6-49ff-b3db-d749f0812ab9"
   },
   "source": [
    "**Extract climatic, geophysical and socio-economic variables from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b5eef13-eb3e-407c-a44a-7c0d98b476fd",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1648569585588,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "5b5eef13-eb3e-407c-a44a-7c0d98b476fd"
   },
   "outputs": [],
   "source": [
    "X_climatic = dataframe[GROUPED_VARS['CLIMATIC VARIABLES']].values\n",
    "X_geo = dataframe[GROUPED_VARS['GEO VARIABLES']].values\n",
    "X_socio = dataframe[GROUPED_VARS['SOCIO VARIABLES']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16023ac4-fc4a-42f5-aaf1-ef6612e443e5",
   "metadata": {
    "id": "16023ac4-fc4a-42f5-aaf1-ef6612e443e5"
   },
   "source": [
    "**Extract Dengue variables from the dataframe, apply a root scaling and normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "816a51e4-c585-4fc6-9620-3227c699164e",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1648569585589,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "816a51e4-c585-4fc6-9620-3227c699164e"
   },
   "outputs": [],
   "source": [
    "y_dengue = dataframe[GROUPED_VARS['DENGUE']].values\n",
    "scaler = MinMaxScaler()\n",
    "y_dengue = scaler.fit_transform(y_dengue)\n",
    "#y_dengue = np.power(y_dengue, 1/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0401b9-59af-4f29-868b-c37052808de9",
   "metadata": {
    "id": "3b0401b9-59af-4f29-868b-c37052808de9"
   },
   "source": [
    "**Apply data reduction technique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b0059-cf6d-469d-ade5-85c86bb5ad36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1648569585589,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "ce6b0059-cf6d-469d-ade5-85c86bb5ad36",
    "outputId": "2856195c-7124-431c-fd80-24332f911911"
   },
   "outputs": [],
   "source": [
    "if DATA_REDUCER_SETTINGS['TYPE'] == 'PLS':    \n",
    "    climatic_vars_reduced = pls_reducer(\n",
    "        X_climatic,\n",
    "        y_dengue,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['CLIMATIC VARIABLES'])\n",
    "    \n",
    "    geo_vars_reduced = pls_reducer(\n",
    "        X_geo,\n",
    "        y_dengue,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['GEO VARIABLES'])\n",
    "    \n",
    "    socio_vars_reduced = pls_reducer(\n",
    "        X_socio,\n",
    "        y_dengue,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['SOCIO VARIABLES'])\n",
    "    \n",
    "elif DATA_REDUCER_SETTINGS['TYPE'] == 'PCA':\n",
    "    climatic_vars_reduced = pca_reducer(\n",
    "        X_climatic,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['CLIMATIC VARIABLES'])\n",
    "    \n",
    "    geo_vars_reduced = pca_reducer(\n",
    "        X_geo,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['GEO VARIABLES'])\n",
    "    \n",
    "    socio_vars_reduced = pca_reducer(\n",
    "        X_socio,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['SOCIO VARIABLES'])\n",
    "else:\n",
    "    print('No data reduction.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ae33ce-0228-4303-a796-ce8a7c50869a",
   "metadata": {
    "id": "52ae33ce-0228-4303-a796-ce8a7c50869a"
   },
   "source": [
    "## Order reduced data in a new dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c063e-4ae5-4371-98ac-def71760765b",
   "metadata": {
    "id": "979c063e-4ae5-4371-98ac-def71760765b"
   },
   "source": [
    "**Normalize remaining variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "zYOskxhcmUDF",
   "metadata": {
    "executionInfo": {
     "elapsed": 491,
     "status": "ok",
     "timestamp": 1648569586077,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "zYOskxhcmUDF"
   },
   "outputs": [],
   "source": [
    "x_excluded = dataframe[GROUPED_VARS['EXCLUDED']].values\n",
    "x_excluded = MinMaxScaler().fit_transform(x_excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b543c83f-80b6-4d4a-9ff6-7188a7ce7b63",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1648569586078,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "b543c83f-80b6-4d4a-9ff6-7188a7ce7b63"
   },
   "outputs": [],
   "source": [
    "X_auxiliar = dataframe[GROUPED_VARS['AUXILIAR']].values\n",
    "X_auxiliar = MinMaxScaler().fit_transform(X_auxiliar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3da58-db5b-456f-a119-0830ad336ced",
   "metadata": {
    "id": "39c3da58-db5b-456f-a119-0830ad336ced"
   },
   "source": [
    "**Create a new database with the reduced, the auxiliar and Dengue variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df37d0b-af3d-4ad6-b17d-5f9f6317f6ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1648569586079,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "1df37d0b-af3d-4ad6-b17d-5f9f6317f6ad",
    "outputId": "b4222e37-e3a9-4a3c-965f-af09c9ee9a69"
   },
   "outputs": [],
   "source": [
    "independent = {'Year':dataframe.Year.values, 'dep_id':dataframe.ADM1_PCODE.values, 't_fundc_ocup18m':np.zeros((len(dataframe.ADM1_PCODE.values))), 't_medioc_ocup18m':np.zeros((len(dataframe.ADM1_PCODE.values))),\n",
    "               'PopTotal_Urban_UF':x_excluded[:, 0], 'PopTotal_Rural_UF':x_excluded[:, 1], 'total_precipitation_d':x_excluded[:, 2], \n",
    "               'surface_pressure_d':x_excluded[:, 3], 'area_km2':x_excluded[:, 4], 'humidity_d':x_excluded[:, 5], 'temperature_2m_d':x_excluded[:, 6],\n",
    "               'min_temperature_2m_d':x_excluded[:, 7]}\n",
    "\n",
    "auxiliar    = {'Month': X_auxiliar[:, 0], \n",
    "               'cases20_99': X_auxiliar[:, 1], 'cases0_19': X_auxiliar[:, 2],\n",
    "               'RandEffects1':  MinMaxScaler().fit_transform(np.reshape(dataframe.ADM1_PCODE.values*dataframe.Month.values, (dataframe.ADM1_PCODE.values.shape[0], 1)))[:,0], \n",
    "               'RandEffects2':  MinMaxScaler().fit_transform(np.reshape(dataframe.ADM1_PCODE.values*dataframe.Year.values, (dataframe.ADM1_PCODE.values.shape[0], 1)))[:,0], \n",
    "               'RandEffects3':  MinMaxScaler().fit_transform(np.reshape(dataframe.ADM1_PCODE.values*dataframe.Month.values*dataframe.Year.values, (dataframe.ADM1_PCODE.values.shape[0], 1)))[:,0]}\n",
    "\n",
    "climatic    = {'PCA0-Climatic':climatic_vars_reduced[:,0], 'PCA1-Climatic':climatic_vars_reduced[:,1], 'PCA2-Climatic':climatic_vars_reduced[:,2], \n",
    "               'PCA3-Climatic':climatic_vars_reduced[:,3]}\n",
    "\n",
    "geo         = {'PCA0-Geo':geo_vars_reduced[:,0], 'PCA1-Geo':geo_vars_reduced[:,1], 'PCA2-Geo':geo_vars_reduced[:,2], \n",
    "               'PCA3-Geo':geo_vars_reduced[:,3], 'PCA4-Geo':geo_vars_reduced[:,4], 'PCA5-Geo':geo_vars_reduced[:,5]}\n",
    "\n",
    "socio       = {'PCA0-Socio':socio_vars_reduced[:,0], 'PCA1-Socio':socio_vars_reduced[:,1], 'PCA2-Socio':socio_vars_reduced[:,2], \n",
    "               'PCA3-Socio':socio_vars_reduced[:,3], 'PCA4-Socio':socio_vars_reduced[:,4], 'PCA5-Socio':socio_vars_reduced[:,5]}\n",
    "\n",
    "dengue      = {'DengRate_all': y_dengue[:,0], 'DengRate_019': y_dengue[:,1]}\n",
    "\n",
    "columns     = {**independent, **auxiliar, **climatic, **geo, **socio, **dengue}\n",
    "\n",
    "reduced_dataframe = pd.DataFrame(columns)\n",
    "reduced_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462125c4-fbb2-4fe7-8cfd-3e023fd5356c",
   "metadata": {
    "id": "462125c4-fbb2-4fe7-8cfd-3e023fd5356c"
   },
   "source": [
    "## Create training and validation data\n",
    "**First of all, the dataframe is divided in two sub-dataframes (training and validation) by using the variable *Year***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb37c08d-4df8-4843-8a40-54877433d299",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1648569586080,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "fb37c08d-4df8-4843-8a40-54877433d299"
   },
   "outputs": [],
   "source": [
    "training_dataframe   = reduced_dataframe[reduced_dataframe.Year <= 2016]\n",
    "validation_dataframe = reduced_dataframe[reduced_dataframe.Year >= 2016]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c096a133-8ecc-479c-a82b-7c4b5ebf0558",
   "metadata": {
    "id": "c096a133-8ecc-479c-a82b-7c4b5ebf0558"
   },
   "source": [
    "**Then the dataset handler is initialized. This object will handle all the operations needed to create, reshape and augment the training and validation dataset to fit the requirements of each Deep Learning or Machine Learning model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "180107d5-4541-4cdc-a6df-6ce7e272bddd",
   "metadata": {
    "executionInfo": {
     "elapsed": 481,
     "status": "ok",
     "timestamp": 1648569586551,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "180107d5-4541-4cdc-a6df-6ce7e272bddd"
   },
   "outputs": [],
   "source": [
    "from datasetHandler import datasetHandler\n",
    "dataset_handler = datasetHandler(training_dataframe, validation_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605b584-8866-4792-acdb-17dfcd613d0d",
   "metadata": {
    "id": "8605b584-8866-4792-acdb-17dfcd613d0d"
   },
   "source": [
    "**Get training and validation vectors from dataframes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b39bd-1397-4376-bd84-06fa3ffadd77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1648569586552,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "f79b39bd-1397-4376-bd84-06fa3ffadd77",
    "outputId": "485eb28f-94a3-446e-d75a-edf87278345f"
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val = dataset_handler.get_data(DATA_PROCESSING_SETTINGS['T LEARNING'], DATA_PROCESSING_SETTINGS['T PREDICTION'])\n",
    "print('\\n\\nX Training shape', x_train.shape)\n",
    "print('Y Training shape', y_train.shape)\n",
    "print('X Validation shape', x_val.shape)\n",
    "print('Y Validation shape', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164e07a3-8236-491c-9c10-6c270cc1d83e",
   "metadata": {
    "id": "164e07a3-8236-491c-9c10-6c270cc1d83e"
   },
   "source": [
    "**Apply data augmention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936103f-fe82-45d0-9ffa-e404d61b96a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1648569586955,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "2936103f-fe82-45d0-9ffa-e404d61b96a6",
    "outputId": "3962c0f0-d479-4188-c18e-e73bba5b627d"
   },
   "outputs": [],
   "source": [
    "x_train_a, y_train_a, x_val_a, y_val_a = dataset_handler.augment(x_train, y_train, x_val, y_val, DATA_PROCESSING_SETTINGS['AUGMENTATION'])\n",
    "print('X Training shape', x_train_a.shape)\n",
    "print('Y Training shape', y_train_a.shape)\n",
    "print('X Validation shape', x_val_a.shape)\n",
    "print('Y Validation shape', y_val_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EG-TYA2Ispib",
   "metadata": {
    "id": "EG-TYA2Ispib"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80-UoybcjRp",
   "metadata": {
    "id": "c80-UoybcjRp"
   },
   "source": [
    "## Baseline\n",
    "**Upload results from Baseline Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hPt4VJr_RVjc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1648569592826,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "hPt4VJr_RVjc",
    "outputId": "cd11c657-d8e0-4326-f9ca-c77a6394643d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PE16': 0, 'PE17': 1, 'PE20': 2, 'PE22': 3, 'PE24': 4, 'PE25': 5}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deps_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FXbD73W0cr5L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1687,
     "status": "ok",
     "timestamp": 1648569595075,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "FXbD73W0cr5L",
    "outputId": "52d0d55d-df0e-4915-cc0d-875c08ea5bed"
   },
   "outputs": [],
   "source": [
    "baselineAll = pd.read_csv(join(config['baseline'], \"Peru\", \"predicted_All_Cases.csv\")).drop('Unnamed: 0', axis=1)\n",
    "baselineAll['Date'] = pd.to_datetime(baselineAll['Date'])\n",
    "\n",
    "cols = ['pred.uci','pred.mean','pred.lci']\n",
    "baselineAll.loc[:, cols] = baselineAll.loc[:, cols].div(baselineAll['PopTotal'], axis=0).multiply(100000.0, axis=0)\n",
    "baselineAll['state_index'] = baselineAll['ADM1_PCODE'].map(deps_dictionary)\n",
    "\n",
    "baselineAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CIT_6Lieq6y4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 536,
     "status": "ok",
     "timestamp": 1648569595604,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "CIT_6Lieq6y4",
    "outputId": "8bfac289-5102-4219-fb1a-856094a1318c"
   },
   "outputs": [],
   "source": [
    "baseline019 = pd.read_csv(join(config['baseline'], \"Peru\", \"predicted_0_19.csv\")).drop('Unnamed: 0', axis=1)\n",
    "baseline019['Date'] = pd.to_datetime(baseline019['Date'])\n",
    "\n",
    "cols = ['pred.uci','pred.mean','pred.lci']\n",
    "baseline019.loc[:, cols] = baseline019.loc[:, cols].div(baseline019['Pop0_19'], axis=0).multiply(100000.0, axis=0)\n",
    "baseline019['state_index'] = baseline019['ADM1_PCODE'].map(deps_dictionary)\n",
    "baseline019.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a956d2f-f784-4d7e-953d-dfac8c36b3ba",
   "metadata": {
    "id": "6a956d2f-f784-4d7e-953d-dfac8c36b3ba",
    "tags": []
   },
   "source": [
    "## CatBoost\n",
    "**Prepare dataset to fit CatBoost requirements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4223e056-f0e6-4f7d-a64a-6e2312c423ad",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1648569602630,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "4223e056-f0e6-4f7d-a64a-6e2312c423ad"
   },
   "outputs": [],
   "source": [
    "trainingC, validationC = dataset_handler.prepare_data_CatBoost(x_train_a[:,:,2:], y_train_a, x_val_a[:,:,2:], y_val_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7656808-1dc4-4ee2-b4b9-dafd208302bd",
   "metadata": {
    "id": "c7656808-1dc4-4ee2-b4b9-dafd208302bd"
   },
   "source": [
    "**Initilize the CatBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9ec7e36-3097-4027-9689-fce5aa650ea6",
   "metadata": {
    "executionInfo": {
     "elapsed": 3334,
     "status": "ok",
     "timestamp": 1648569605962,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "d9ec7e36-3097-4027-9689-fce5aa650ea6"
   },
   "outputs": [],
   "source": [
    "from models import CatBoostNet\n",
    "cat_boost = CatBoostNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb2420-a30c-44cd-bc0c-cc4dde58a263",
   "metadata": {
    "id": "25bb2420-a30c-44cd-bc0c-cc4dde58a263"
   },
   "source": [
    "**Train or Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "OpXZZV2VU0va",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1648569605963,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "OpXZZV2VU0va"
   },
   "outputs": [],
   "source": [
    "def compute_scores(gt, preds):\n",
    "    # Compute Normalized-RMSE and R2 metrics\n",
    "    rmse = mean_squared_error(gt, preds, squared=False)\n",
    "    nrmse = rmse/(gt.max() - gt.min())\n",
    "    r2 = r2_score(gt, preds)\n",
    "    return nrmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ad018-27ff-45fc-8f3d-f2bb92bb2c82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 13502,
     "status": "ok",
     "timestamp": 1648458309751,
     "user": {
      "displayName": "Alessandro Sebastianelli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgVWBLvEG9yF5gq_rkIyj5MCLQv1OzgwlLWxu7e=s64",
      "userId": "16665723407518839496"
     },
     "user_tz": -120
    },
    "id": "d87ad018-27ff-45fc-8f3d-f2bb92bb2c82",
    "outputId": "9f173e5c-7e91-41bb-f4d4-fd807d9b3df9"
   },
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "nb_deps = 6\n",
    "\n",
    "if TRAINING:\n",
    "    # get model trained on Brazil\n",
    "    cat_boost_model = glob(join(config['output'], \"Brazil\", \"catboost*\"))\n",
    "    if cat_boost_model == []:\n",
    "        print('No file with such pattern was found in the directory.')\n",
    "    else:\n",
    "        cat_boost.load(cat_boost_model[0])\n",
    "        \n",
    "        # finetune pre-trained model in Peru\n",
    "        cat_boost.train(trainingC, validationC, output_path=join(config['output'], \"Peru\"))\n",
    "\n",
    "else:    \n",
    "    # retrieve finetuned model\n",
    "    cat_boost_model_finetuned = glob(join(config['output'], \"Peru\", \"catboost*\"))\n",
    "    if cat_boost_model_finetuned == []:\n",
    "        print('No file with such pattern was found in the directory. Run TRAINING = True first.')\n",
    "    else:\n",
    "        cat_boost.load(cat_boost_model_finetuned[0])\n",
    "\n",
    "        trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "        preds_tra = cat_boost.model.predict(trainC[0])\n",
    "        preds_tra[preds_tra < 0] = 0\n",
    "        preds_val = cat_boost.model.predict(valC[0])\n",
    "        preds_val[preds_val < 0] = 0\n",
    "\n",
    "        datelist = pd.date_range('01-01-2010', end='01-01-2020', freq='YS')\n",
    "        dates = datelist.strftime(\"%Y\").tolist()\n",
    "\n",
    "        # Total timesteps (nb of months inside this time window) = 228\n",
    "        month_datelist = pd.date_range('01-01-2010', end='31-12-2019', freq='MS')\n",
    "        ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "        res = []\n",
    "        res.append('Department,NRMSE 0-19 Training,NRMSE All Training,NRMSE 0-19 Validation,NRMSE All Validation')\n",
    "\n",
    "        for i in range(nb_deps):\n",
    "            fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18,9))\n",
    "\n",
    "            idxt = preds_tra.shape[0]//nb_deps\n",
    "            idxv = preds_val.shape[0]//nb_deps\n",
    "\n",
    "            t1 = ts[12:12+idxt]\n",
    "            t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "            # Reverting Data Norm ---------\n",
    "            pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "            pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "            gtt = scaler.inverse_transform(trainC[1][idxt*i:idxt*(i+1), :])\n",
    "            gtv = scaler.inverse_transform(valC[1][idxv*i:idxv*(i+1), :])\n",
    "            # ------------------------------\n",
    "\n",
    "            baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "            base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "            axes[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "            axes[0].plot(t1, pt[:,0], '--', color='red', label = 'CatBoost prediction on Training data')\n",
    "            axes[0].plot(t2, pv[:,0], '-', color='red', label = 'CatBoost prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            axes[1].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "            axes[1].plot(t1, pt[:,1], '--', color='red', label = 'CatBoost prediction on Training data')\n",
    "            axes[1].plot(t2, pv[:,1], '-', color='red', label = 'CatBoost prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[1].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[1].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            # Configure visualization\n",
    "            axes[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left')\n",
    "            axes[0].set(ylabel='DIR Total Population')\n",
    "            axes[1].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "            axes[1].legend(ncol=3, loc='lower center', bbox_to_anchor=(0.5,-0.5), shadow=True)\n",
    "            axes[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "            axes[1].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "            for ax in axes:\n",
    "                #ax.set_xticks(np.arange(0, idxt+idxv, 11))\n",
    "                ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "                ax.grid(True, linewidth=0.5)\n",
    "                ax.set_xlim(ts[0],ts[-1]+5)\n",
    "            axes[0].set_xticklabels([])\n",
    "            axes[1].set_xticklabels(dates)\n",
    "\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            print('--------------------- TRAINING scores ---------------------')\n",
    "            rmse1, r2 = compute_scores(gtt[:,0], pt[:,0])\n",
    "            print('N-RMSE (group total): ', rmse1)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse2, r2 = compute_scores(gtt[:,1], pt[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse2)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            print('-------------------- VALIDATION scores ---------------------')\n",
    "            rmse3, r2 = compute_scores(gtv[:,0], pv[:,0])\n",
    "            print('N-RMSE (group total): ', rmse3)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse4, r2 = compute_scores(gtv[:,1], pv[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse4)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            res.append('{},{},{},{},{}'.format(DEP_NAMES[i], rmse2, rmse1, rmse4, rmse3))\n",
    "\n",
    "            print('\\n##########################################################################################################')\n",
    "\n",
    "        # Save results\n",
    "        today = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
    "        with open(join(config['metrics'], \"Peru\", 'catboost-'+today+'.csv'), 'w') as f:\n",
    "            print('Writing performance metrics to file...')\n",
    "            for item in res:\n",
    "                f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Le8KHGbJ2MuH",
   "metadata": {
    "id": "Le8KHGbJ2MuH"
   },
   "source": [
    "Write file with performance metrics for baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oJpCRde21sFh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1648458310434,
     "user": {
      "displayName": "Alessandro Sebastianelli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgVWBLvEG9yF5gq_rkIyj5MCLQv1OzgwlLWxu7e=s64",
      "userId": "16665723407518839496"
     },
     "user_tz": -120
    },
    "id": "oJpCRde21sFh",
    "outputId": "fa1459b1-7673-49d6-c301-1e37b336f925"
   },
   "outputs": [],
   "source": [
    "cat_boost_model_finetuned = glob(join(config['output'], \"Peru\", \"catboost*\"))[0]\n",
    "cat_boost.load(cat_boost_model_finetuned)\n",
    "\n",
    "trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "preds_tra = cat_boost.model.predict(trainC[0])\n",
    "preds_tra[preds_tra < 0] = 0\n",
    "preds_val = cat_boost.model.predict(valC[0])\n",
    "preds_val[preds_val < 0] = 0\n",
    "\n",
    "datelist = pd.date_range('01-01-2010', end='01-01-2020', freq='YS')\n",
    "dates = datelist.strftime(\"%Y\").tolist()\n",
    "# Total timesteps (nb of months inside this time window) = 228\n",
    "month_datelist = pd.date_range('01-01-2010', end='31-12-2019', freq='MS')\n",
    "ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "res = []\n",
    "res.append('Department,NRMSE 0-19 Training,NRMSE All Training,NRMSE 0-19 Validation,NRMSE All Validation')\n",
    "\n",
    "for i in range(nb_deps):\n",
    "    idxt = preds_tra.shape[0]//nb_deps\n",
    "    idxv = preds_val.shape[0]//nb_deps \n",
    "\n",
    "    t1 = ts[12:12+idxt]\n",
    "    t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "    # Reverting Data Norm ---------\n",
    "    gtt = scaler.inverse_transform(trainC[1][idxt*i:idxt*(i+1), :])\n",
    "    gtv = scaler.inverse_transform(valC[1][idxv*i:idxv*(i+1), :])\n",
    "    # ------------------------------\n",
    "    \n",
    "    baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "    base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "    print('--------------------- TRAINING scores ---------------------')\n",
    "    rmse1, r2 = compute_scores(gtt[:,0], baseAll_df['pred.mean'].iloc[t1])\n",
    "    print('N-RMSE (group total): ', rmse1)\n",
    "    print('R2   (group total): ', r2)\n",
    "\n",
    "    rmse2, r2 = compute_scores(gtt[:,1], base019_df['pred.mean'].iloc[t1])\n",
    "    print('N-RMSE (group 0-19): ', rmse2)\n",
    "    print('R2   (group 0-19): ', r2)\n",
    "\n",
    "    print('-------------------- VALIDATION scores ---------------------')\n",
    "    rmse3, r2 = compute_scores(gtv[:,0], baseAll_df['pred.mean'].iloc[t2])\n",
    "    print('N-RMSE (group total): ', rmse3)\n",
    "    print('R2   (group total): ', r2)\n",
    "\n",
    "    rmse4, r2 = compute_scores(gtv[:,1], base019_df['pred.mean'].iloc[t2])\n",
    "    print('N-RMSE (group 0-19): ', rmse4)\n",
    "    print('R2   (group 0-19): ', r2)\n",
    "\n",
    "    res.append('{},{},{},{},{}'.format(DEP_NAMES[i], rmse2, rmse1, rmse4, rmse3))\n",
    "    \n",
    "    print('\\n##########################################################################################################')\n",
    "\n",
    "# Save results\n",
    "today = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
    "with open(join(config['metrics'], \"Peru\", 'baseline-'+today+'.csv'),'w') as f:\n",
    "    print('Writing performance metrics to file...')\n",
    "    for item in res:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E3iGL37YB9FU",
   "metadata": {
    "id": "E3iGL37YB9FU"
   },
   "source": [
    "## SVM\n",
    "**It uses the same data structure of CatBoost, no need to prepare data**\n",
    "\n",
    "**Initilize the SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "P0TfzBbeB8yl",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1648569641364,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "P0TfzBbeB8yl"
   },
   "outputs": [],
   "source": [
    "from models import SVMNet\n",
    "svm = SVMNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mho-Gh3HDan5",
   "metadata": {
    "id": "mho-Gh3HDan5"
   },
   "source": [
    "**Train or Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZXFfioFlC9-9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9373,
     "status": "ok",
     "timestamp": 1648458320051,
     "user": {
      "displayName": "Alessandro Sebastianelli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgVWBLvEG9yF5gq_rkIyj5MCLQv1OzgwlLWxu7e=s64",
      "userId": "16665723407518839496"
     },
     "user_tz": -120
    },
    "id": "ZXFfioFlC9-9",
    "outputId": "61b6fe77-0ca3-458b-831c-23acf9852efd"
   },
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "\n",
    "if TRAINING:\n",
    "    \n",
    "    # get model trained on Brazil\n",
    "    svm_model = glob(join(config['output'], \"Brazil\", \"svm*\"))\n",
    "    if svm_model == []:\n",
    "        print('No file with such pattern was found in the directory.')\n",
    "    else:\n",
    "        svm.load(svm_model[0])\n",
    "        \n",
    "        # finetune pre-trained model in Peru\n",
    "        svm.train(trainingC, validationC, output_path=join(config['output'], \"Peru\"))\n",
    "\n",
    "else:\n",
    "    # retrieve finetuned model\n",
    "    svm_model_finetuned = glob(join(config['output'], \"Peru\", \"svm*\"))\n",
    "    if svm_model_finetuned == []:\n",
    "        print('No file with such pattern was found in the directory. Run TRAINING = True first.')\n",
    "    else:\n",
    "        svm.load(svm_model_finetuned[0])\n",
    "\n",
    "        trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "        preds_tra = svm.model.predict(trainC[0])\n",
    "        preds_tra[preds_tra < 0] = 0\n",
    "        preds_val = svm.model.predict(valC[0])\n",
    "        preds_val[preds_val < 0] = 0\n",
    "\n",
    "        datelist = pd.date_range('01-01-2010', end='01-01-2020', freq='YS')\n",
    "        dates = datelist.strftime(\"%Y\").tolist()\n",
    "        month_datelist = pd.date_range('01-01-2010', end='31-12-2019', freq='MS')\n",
    "        ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "        res = []\n",
    "        res.append('Department,NRMSE 0-19 Training,NRMSE All Training,NRMSE 0-19 Validation,NRMSE All Validation')\n",
    "\n",
    "        for i in range(nb_deps):\n",
    "            fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18,9))\n",
    "\n",
    "            idxt = preds_tra.shape[0]//nb_deps\n",
    "            idxv = preds_val.shape[0]//nb_deps\n",
    "\n",
    "            t1 = ts[12:12+idxt]\n",
    "            t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "            # Reverting Data Norm ---------\n",
    "            pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "            pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "            gtt = scaler.inverse_transform(trainC[1][idxt*i:idxt*(i+1), :])\n",
    "            gtv = scaler.inverse_transform(valC[1][idxv*i:idxv*(i+1), :])\n",
    "            # ------------------------------\n",
    "\n",
    "            baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "            base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "            axes[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "            axes[0].plot(t1, pt[:,0], '--', color='red', label = 'SVM prediction on Training data')\n",
    "            axes[0].plot(t2, pv[:,0], '-', color='red', label = 'SVM prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            axes[1].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "            axes[1].plot(t1, pt[:,1], '--', color='red', label = 'SVM prediction on Training data')\n",
    "            axes[1].plot(t2, pv[:,1], '-', color='red', label = 'SVM prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[1].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[1].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            # Configure visualization\n",
    "            axes[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left')\n",
    "            axes[0].set(ylabel='DIR Total Population')\n",
    "            axes[1].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "            axes[1].legend(ncol=3, loc='lower center', bbox_to_anchor=(0.5,-0.5), shadow=True)\n",
    "            axes[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "            axes[1].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "            for ax in axes:\n",
    "                ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "                ax.grid(True, linewidth=0.5)\n",
    "                ax.set_xlim(ts[0],ts[-1]+5)\n",
    "            axes[0].set_xticklabels([])\n",
    "            axes[1].set_xticklabels(dates)\n",
    "\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            print('--------------------- TRAINING scores ---------------------')\n",
    "            rmse1, r2 = compute_scores(gtt[:,0], pt[:,0])\n",
    "            print('N-RMSE (group total): ', rmse1)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse2, r2 = compute_scores(gtt[:,1], pt[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse2)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            print('-------------------- VALIDATION scores ---------------------')\n",
    "            rmse3, r2 = compute_scores(gtv[:,0], pv[:,0])\n",
    "            print('N-RMSE (group total): ', rmse3)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse4, r2 = compute_scores(gtv[:,1], pv[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse4)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            res.append('{},{},{},{},{}'.format(DEP_NAMES[i], rmse2, rmse1, rmse4, rmse3))\n",
    "\n",
    "            print('\\n##########################################################################################################')\n",
    "\n",
    "        # Save results\n",
    "        today = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
    "        with open(join(config['metrics'], \"Peru\", 'svm-'+today+'.csv'), 'w') as f:\n",
    "            print('Writing performance metrics to file...')\n",
    "            for item in res:\n",
    "                f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nhxwEojupvjg",
   "metadata": {
    "id": "nhxwEojupvjg",
    "tags": []
   },
   "source": [
    "## LSTM\n",
    "**Prepare dataset to fit LSTM requirements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "WxW7x16epvji",
   "metadata": {
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1648569646402,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "WxW7x16epvji"
   },
   "outputs": [],
   "source": [
    "trainingL, validationL = dataset_handler.prepare_data_LSTM(x_train_a[:,:,2:], y_train_a, x_val_a[:,:,2:], y_val_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZMlQADOWpvjl",
   "metadata": {
    "id": "ZMlQADOWpvjl"
   },
   "source": [
    "**Initilize the LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "En62JMKxpvjo",
   "metadata": {
    "executionInfo": {
     "elapsed": 5152,
     "status": "ok",
     "timestamp": 1648569651939,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "En62JMKxpvjo"
   },
   "outputs": [],
   "source": [
    "from models import LSTMNet\n",
    "lstm = LSTMNet(trainingL[0].shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wBlGgF1Npvjq",
   "metadata": {
    "id": "wBlGgF1Npvjq"
   },
   "source": [
    "**Train or Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OX-zrXn8pvjs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 12515,
     "status": "ok",
     "timestamp": 1648458339140,
     "user": {
      "displayName": "Alessandro Sebastianelli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgVWBLvEG9yF5gq_rkIyj5MCLQv1OzgwlLWxu7e=s64",
      "userId": "16665723407518839496"
     },
     "user_tz": -120
    },
    "id": "OX-zrXn8pvjs",
    "outputId": "e46135bd-a684-433e-e51e-8e2b6a025a31"
   },
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "\n",
    "if TRAINING:\n",
    "    \n",
    "    # get model trained on Brazil\n",
    "    lstm_model = glob(join(config['output'], \"Brazil\", \"lstm*\"))\n",
    "    if lstm_model == []:\n",
    "        print('No file with such pattern was found in the directory.')\n",
    "    else:\n",
    "        lstm.load(lstm_model[0])\n",
    "        \n",
    "        # finetune pre-trained model in Peru\n",
    "        lstm.train(trainingL, validationL, output_path=join(config['output'], \"Peru\"))\n",
    "\n",
    "else:\n",
    "    # retrieve finetuned model\n",
    "    lstm_model_finetuned = glob(join(config['output'], \"Peru\", \"lstm*\"))\n",
    "    if lstm_model_finetuned == []:\n",
    "        print('No file with such pattern was found in the directory. Run TRAINING = True first.')\n",
    "    else:\n",
    "        lstm.load(lstm_model_finetuned[0])\n",
    "\n",
    "        trainL, valL = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "        preds_tra = lstm.model.predict(trainL[0])\n",
    "        preds_tra[preds_tra < 0] = 0\n",
    "        preds_val = lstm.model.predict(valL[0])\n",
    "        preds_val[preds_val < 0] = 0\n",
    "\n",
    "        #datelist = pd.date_range('01-01-2001', end='31-12-2019', freq = 'Y')\n",
    "        datelist = pd.date_range('01-01-2010', end='01-01-2020', freq='YS')\n",
    "        dates = datelist.strftime(\"%Y\").tolist()\n",
    "        month_datelist = pd.date_range('01-01-2010', end='31-12-2019', freq='MS')\n",
    "        ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "        res = []\n",
    "        res.append('Department,NRMSE 0-19 Training,NRMSE All Training,NRMSE 0-19 Validation,NRMSE All Validation')\n",
    "\n",
    "        for i in range(nb_deps):\n",
    "            fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18,9))\n",
    "\n",
    "            idxt = preds_tra.shape[0]//nb_deps\n",
    "            idxv = preds_val.shape[0]//nb_deps\n",
    "\n",
    "            t1 = ts[12:12+idxt]\n",
    "            t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "            # Reverting Data Norm ---------\n",
    "            pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "            pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "            gtt = scaler.inverse_transform(trainL[1][idxt*i:idxt*(i+1), :])\n",
    "            gtv = scaler.inverse_transform(valL[1][idxv*i:idxv*(i+1), :])\n",
    "            # ------------------------------\n",
    "\n",
    "            baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "            base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "            axes[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "            axes[0].plot(t1, pt[:,0], '--', color='red', label = 'LSTM prediction on Training data')\n",
    "            axes[0].plot(t2, pv[:,0], '-', color='red', label = 'LSTM prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            axes[1].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "            axes[1].plot(t1, pt[:,1], '--', color='red', label = 'LSTM prediction on Training data')\n",
    "            axes[1].plot(t2, pv[:,1], '-', color='red', label = 'LSTM prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[1].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[1].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            # Configure visualization\n",
    "            axes[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left')\n",
    "            axes[0].set(ylabel='DIR Total Population')\n",
    "            axes[1].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "            axes[1].legend(ncol=3, loc='lower center', bbox_to_anchor=(0.5,-0.5), shadow=True)\n",
    "            axes[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "            axes[1].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "            for ax in axes:\n",
    "                ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "                ax.grid(True, linewidth=0.5)\n",
    "                ax.set_xlim(ts[0],ts[-1]+5)\n",
    "            axes[0].set_xticklabels([])\n",
    "            axes[1].set_xticklabels(dates)\n",
    "\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            print('--------------------- TRAINING scores ---------------------')\n",
    "            rmse1, r2 = compute_scores(gtt[:,0], pt[:,0])\n",
    "            print('N-RMSE (group total): ', rmse1)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse2, r2 = compute_scores(gtt[:,1], pt[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse2)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            print('-------------------- VALIDATION scores ---------------------')\n",
    "            rmse3, r2 = compute_scores(gtv[:,0], pv[:,0])\n",
    "            print('N-RMSE (group total): ', rmse3)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse4, r2 = compute_scores(gtv[:,1], pv[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse4)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            res.append('{},{},{},{},{}'.format(DEP_NAMES[i], rmse2, rmse1, rmse4, rmse3))\n",
    "\n",
    "            print('\\n##########################################################################################################')\n",
    "\n",
    "        # Save results\n",
    "        today = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
    "        with open(join(config['metrics'], \"Peru\", 'lstm-'+today+'.csv'), 'w') as f:\n",
    "            print('Writing performance metrics to file...')\n",
    "            for item in res:\n",
    "                f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oAK1FjNaPrpZ",
   "metadata": {
    "id": "oAK1FjNaPrpZ",
    "tags": []
   },
   "source": [
    "## Ensemble\n",
    "**Prepare dataset to fit the Ensemble requirements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "O0dFRcYMhCDP",
   "metadata": {
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1648569655298,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "O0dFRcYMhCDP"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eoh0x3-2i6G3",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1648569655798,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "eoh0x3-2i6G3"
   },
   "outputs": [],
   "source": [
    "def interpolation(trainL, valL):\n",
    "\n",
    "    def pol_zero(x, a):\n",
    "        return a \n",
    "\n",
    "    def pol_one(x, a, b):\n",
    "        return a + b*x\n",
    "\n",
    "    def pol_two(x, a, b, c):\n",
    "        return a + b*x + c*x*x  \n",
    "\n",
    "    train_019_pol0 = []\n",
    "    train_019_pol1 = []\n",
    "    train_019_pol2 = []\n",
    "\n",
    "    train_all_pol0 = []\n",
    "    train_all_pol1 = []\n",
    "    train_all_pol2 = []\n",
    "\n",
    "    val_019_pol0 = []\n",
    "    val_019_pol1 = []\n",
    "    val_019_pol2 = []\n",
    "\n",
    "    val_all_pol0 = []\n",
    "    val_all_pol1 = []\n",
    "    val_all_pol2 = []\n",
    "\n",
    "    for i in tqdm(range(trainL[0].shape[0])):\n",
    "        popt_0, _ = curve_fit(pol_zero, np.arange(0,12,1), trainL[0][i,:,-1])\n",
    "        popt_1, _ = curve_fit(pol_one, np.arange(0,12,1),  trainL[0][i,:,-1])\n",
    "        popt_2, _ = curve_fit(pol_two, np.arange(0,12,1), trainL[0][i,:,-1])\n",
    "\n",
    "        train_019_pol0.append(pol_zero(12, *popt_0))\n",
    "        train_019_pol1.append(pol_one(12, *popt_1))\n",
    "        train_019_pol2.append(pol_two(12, *popt_2))\n",
    "\n",
    "        popt_0, _ = curve_fit(pol_zero, np.arange(0,12,1), trainL[0][i,:,-2])\n",
    "        popt_1, _ = curve_fit(pol_one, np.arange(0,12,1),  trainL[0][i,:,-2])\n",
    "        popt_2, _ = curve_fit(pol_two, np.arange(0,12,1),  trainL[0][i,:,-2])\n",
    "\n",
    "        train_all_pol0.append(pol_zero(12, *popt_0))\n",
    "        train_all_pol1.append(pol_one(12, *popt_1))\n",
    "        train_all_pol2.append(pol_two(12, *popt_2))\n",
    "\n",
    "    for i in tqdm(range(valL[0].shape[0])):\n",
    "        popt_0, _ = curve_fit(pol_zero, np.arange(0,12,1), valL[0][i,:,-1])\n",
    "        popt_1, _ = curve_fit(pol_one, np.arange(0,12,1), valL[0][i,:,-1])\n",
    "        popt_2, _ = curve_fit(pol_two, np.arange(0,12,1), valL[0][i,:,-1])\n",
    "\n",
    "        val_019_pol0.append(pol_zero(12, *popt_0))\n",
    "        val_019_pol1.append(pol_one(12, *popt_1))\n",
    "        val_019_pol2.append(pol_two(12, *popt_2))\n",
    "\n",
    "        popt_0, _ = curve_fit(pol_zero, np.arange(0,12,1), valL[0][i,:,-2])\n",
    "        popt_1, _ = curve_fit(pol_one, np.arange(0,12,1), valL[0][i,:,-2])\n",
    "        popt_2, _ = curve_fit(pol_two, np.arange(0,12,1), valL[0][i,:,-2])\n",
    "\n",
    "        val_all_pol0.append(pol_zero(12, *popt_0))\n",
    "        val_all_pol1.append(pol_one(12, *popt_1))\n",
    "        val_all_pol2.append(pol_two(12, *popt_2))\n",
    "\n",
    "    train_019_pol0 = np.array(train_019_pol0)\n",
    "    train_019_pol1 = np.array(train_019_pol1)\n",
    "    train_019_pol2 = np.array(train_019_pol2)\n",
    "\n",
    "    train_all_pol0 = np.array(train_all_pol0)\n",
    "    train_all_pol1 = np.array(train_all_pol1)\n",
    "    train_all_pol2 = np.array(train_all_pol2)\n",
    "\n",
    "    val_019_pol0 = np.array(val_019_pol0)\n",
    "    val_019_pol1 = np.array(val_019_pol1)\n",
    "    val_019_pol2 = np.array(val_019_pol2)\n",
    "\n",
    "    val_all_pol0 = np.array(val_all_pol0)\n",
    "    val_all_pol1 = np.array(val_all_pol1)\n",
    "    val_all_pol2 = np.array(val_all_pol2)\n",
    "\n",
    "\n",
    "    train_pol0 = np.column_stack((train_019_pol0, train_all_pol0))\n",
    "    train_pol1 = np.column_stack((train_019_pol1, train_all_pol1))\n",
    "    train_pol2 = np.column_stack((train_019_pol2, train_all_pol2))\n",
    "\n",
    "    val_pol0 = np.column_stack((val_019_pol0, val_all_pol0))\n",
    "    val_pol1 = np.column_stack((val_019_pol1, val_all_pol1))\n",
    "    val_pol2 = np.column_stack((val_019_pol2, val_all_pol2))\n",
    "\n",
    "    return train_pol0, train_pol1, train_pol2, val_pol0, val_pol1, val_pol2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9EhJ44gxPrpb",
   "metadata": {
    "id": "9EhJ44gxPrpb"
   },
   "source": [
    "**Initilize the Ensemble model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "GX4eBGHH0LKS",
   "metadata": {
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1648569660855,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "GX4eBGHH0LKS"
   },
   "outputs": [],
   "source": [
    "from models import Ensamble, CatBoostEnsableNet, RandomForestEnsableNet\n",
    "ens = RandomForestEnsableNet(True) #CatBoostEnsableNet() #Ensemble(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waM5x7G4Prpe",
   "metadata": {
    "id": "waM5x7G4Prpe"
   },
   "source": [
    "**Train or Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QeQEC7APXM2w",
   "metadata": {
    "id": "QeQEC7APXM2w"
   },
   "outputs": [],
   "source": [
    "# Load Brazilian inner models\n",
    "cat_boost.load(glob(join(config['output'], \"Brazil\", \"catboost*\"))[0])\n",
    "svm.load(glob(join(config['output'], \"Brazil\", \"svm*\"))[0])\n",
    "lstm.load(glob(join(config['output'], \"Brazil\", \"lstm*\"))[0])\n",
    "\n",
    "# Load finetuned inner models\n",
    "# cat_boost.load(glob(join(config['output'], \"Peru\", \"catboost*\"))[0])\n",
    "# svm.load(glob(join(config['output'], \"Peru\", \"svm*\"))[0])\n",
    "# lstm.load(glob(join(config['output'], \"Peru\", \"lstm*\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oKKkq04-Prpf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 13330,
     "status": "ok",
     "timestamp": 1648458480679,
     "user": {
      "displayName": "Alessandro Sebastianelli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgVWBLvEG9yF5gq_rkIyj5MCLQv1OzgwlLWxu7e=s64",
      "userId": "16665723407518839496"
     },
     "user_tz": -120
    },
    "id": "oKKkq04-Prpf",
    "outputId": "09dd77e3-a527-4b75-ff2c-15122d32c181"
   },
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "\n",
    "if TRAINING:\n",
    "    #ens.train([catBoost_train, svm_train, lstm_train], trainingL[1], \n",
    "    #          [catBoost_val, svm_val, lstm_val], validationL[1], output_path=join(config['output'], \"Peru\"))\n",
    "    \n",
    "    # get model trained on Brazil\n",
    "    ens_model = glob(join(config['output'], \"Brazil\", \"rf*\"))\n",
    "    if ens_model == []:\n",
    "        print('No file with such pattern was found in the directory.')\n",
    "    else:\n",
    "        ens.load(ens_model[0])\n",
    "        \n",
    "        catBoost_train = cat_boost.model.predict(trainingC[0])\n",
    "        catBoost_train[catBoost_train < 0] = 0\n",
    "        catBoost_val = cat_boost.model.predict(validationC[0])\n",
    "        catBoost_val[catBoost_val < 0] = 0\n",
    "\n",
    "        svm_train = svm.model.predict(trainingC[0])\n",
    "        svm_train[svm_train < 0] = 0\n",
    "        svm_val = svm.model.predict(validationC[0])\n",
    "        svm_val[svm_val < 0] = 0\n",
    "\n",
    "        lstm_train = lstm.model.predict(trainingL[0])\n",
    "        lstm_train[lstm_train < 0] = 0\n",
    "        lstm_val = lstm.model.predict(validationL[0])\n",
    "        lstm_val[lstm_val < 0] = 0\n",
    "\n",
    "        train_pol0, train_pol1, train_pol2, val_pol0, val_pol1, val_pol2 = interpolation(trainingL, validationL)\n",
    "\n",
    "        x_trainE = np.concatenate((catBoost_train, svm_train, lstm_train, train_pol0, train_pol1, train_pol2), axis=-1)\n",
    "        y_trainE = trainingL[1]\n",
    "        x_valE = np.concatenate((catBoost_val, svm_val, lstm_val, val_pol0, val_pol1, val_pol2), axis=-1)\n",
    "        y_valE = validationL[1]\n",
    "\n",
    "        ens.train(x_trainE, y_trainE, x_valE, y_valE, output_path=join(config['output'], \"Peru\"))\n",
    "\n",
    "else:\n",
    "    # retrieve finetuned RF model\n",
    "    ens_model_finetuned = glob(join(config['output'], \"Peru\", \"rf*\"))\n",
    "    if ens_model_finetuned == []:\n",
    "        print('No file with such pattern was found in the directory. Run TRAINING = True first.')\n",
    "    else:\n",
    "        ens.load(ens_model_finetuned[0])\n",
    "    \n",
    "        trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "        trainL, valL = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "        catBoost_train = cat_boost.model.predict(trainC[0])\n",
    "        catBoost_train[catBoost_train < 0] = 0\n",
    "        catBoost_val = cat_boost.model.predict(valC[0])\n",
    "        catBoost_val[catBoost_val < 0] = 0\n",
    "\n",
    "        svm_train = svm.model.predict(trainC[0])\n",
    "        svm_train[svm_train < 0] = 0\n",
    "        svm_val = svm.model.predict(valC[0])\n",
    "        svm_val[svm_val < 0] = 0\n",
    "\n",
    "        lstm_train = lstm.model.predict(trainL[0])\n",
    "        lstm_train[lstm_train < 0] = 0\n",
    "        lstm_val = lstm.model.predict(valL[0])\n",
    "        lstm_val[lstm_val < 0] = 0\n",
    "\n",
    "        train_pol0, train_pol1, train_pol2, val_pol0, val_pol1, val_pol2 = interpolation(trainL, valL)\n",
    "\n",
    "        x_trainE = np.concatenate((catBoost_train, svm_train, lstm_train, train_pol0, train_pol1, train_pol2), axis=-1)\n",
    "        y_trainE = trainingL[1]\n",
    "        x_valE = np.concatenate((catBoost_val, svm_val, lstm_val, val_pol0, val_pol1, val_pol2), axis=-1)\n",
    "        y_valE = validationL[1]\n",
    "\n",
    "        preds_tra = ens.model.predict(x_trainE)\n",
    "        preds_tra[preds_tra < 0] = 0\n",
    "        preds_val = ens.model.predict(x_valE)\n",
    "        preds_val[preds_val < 0] = 0\n",
    "\n",
    "        datelist = pd.date_range('01-01-2010', end='01-01-2020', freq='YS')\n",
    "        dates = datelist.strftime(\"%Y\").tolist()\n",
    "        # Total timesteps (nb of months inside this time window) = 228\n",
    "        month_datelist = pd.date_range('01-01-2010', end='31-12-2019', freq='MS')\n",
    "        ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "        res = []\n",
    "        res.append('Department,NRMSE 0-19 Training,NRMSE All Training,NRMSE 0-19 Validation,NRMSE All Validation')\n",
    "\n",
    "        for i in range(nb_deps):\n",
    "            fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18,9))\n",
    "\n",
    "            idxt = preds_tra.shape[0]//nb_deps\n",
    "            idxv = preds_val.shape[0]//nb_deps        \n",
    "\n",
    "            t1 = ts[12:12+idxt]\n",
    "            t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "            # Reverting Data Norm ---------\n",
    "            pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "            pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "            gtt = scaler.inverse_transform(trainL[1][idxt*i:idxt*(i+1), :])\n",
    "            gtv = scaler.inverse_transform(valL[1][idxv*i:idxv*(i+1), :])\n",
    "            # ------------------------------\n",
    "\n",
    "            baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "            base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "            axes[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "            axes[0].plot(t1, pt[:,0], '--', color='red', label = 'Ensemble prediction on Training data')\n",
    "            axes[0].plot(t2, pv[:,0], '-', color='red', label = 'Ensemble prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            axes[1].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "            axes[1].plot(t1, pt[:,1], '--', color='red', label = 'Ensemble prediction on Training data')\n",
    "            axes[1].plot(t2, pv[:,1], '-', color='red', label = 'Ensemble prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[1].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[1].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            # Configure visualization\n",
    "            axes[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left')\n",
    "            axes[0].set(ylabel='DIR Total Population')\n",
    "            axes[1].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "            axes[1].legend(ncol=3, loc='lower center', bbox_to_anchor=(0.5,-0.5), shadow=True)\n",
    "            axes[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "            axes[1].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "            for ax in axes:\n",
    "                ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "                ax.grid(True, linewidth=0.5)\n",
    "                ax.set_xlim(ts[0],ts[-1]+5)\n",
    "            axes[0].set_xticklabels([])\n",
    "            axes[1].set_xticklabels(dates)\n",
    "\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            #print('--------------------- TRAINING scores ---------------------')\n",
    "            #rmse1, r2 = compute_scores(gtt[:,0], pt[:,0])\n",
    "            #print('N-RMSE (group total): ', rmse1)\n",
    "            #print('R2   (group total): ', r2)\n",
    "\n",
    "            #rmse2, r2 = compute_scores(gtt[:,1], pt[:,1])\n",
    "            #print('N-RMSE (group 0-19): ', rmse2)\n",
    "            #print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            print('-------------------- VALIDATION scores Proposed ---------------------')\n",
    "            rmse3, r2 = compute_scores(gtv[:,0], pv[:,0])\n",
    "            print('N-RMSE (group total): ', rmse3)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse4, r2 = compute_scores(gtv[:,1], pv[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse4)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            print('-------------------- VALIDATION scores Baseline ---------------------')\n",
    "            rmse3, r2 = compute_scores(gtv[:,0], baseAll_df['pred.mean'].iloc[t2])\n",
    "            print('N-RMSE (group total): ', rmse3)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse4, r2 = compute_scores(gtv[:,1], base019_df['pred.mean'].iloc[t2])\n",
    "            print('N-RMSE (group 0-19): ', rmse4)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            #res.append('{},{},{},{},{}'.format(DEP_NAMES[i], rmse2, rmse1, rmse4, rmse3))\n",
    "\n",
    "            print('\\n##########################################################################################################')\n",
    "\n",
    "        # Save results\n",
    "        #today = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
    "        # with open(join(config['metrics'], \"Peru\", 'rf_ensemble-'+today+'.csv'), 'w') as f:\n",
    "        #     print('Writing performance metrics to file...')\n",
    "        #     for item in res:\n",
    "        #         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FAj0jXtZy1sc",
   "metadata": {
    "id": "FAj0jXtZy1sc"
   },
   "source": [
    "Compute confidence intervals for the ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a4c6f-6478-4bbd-ba18-9620ae38cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Brazilian models\n",
    "cat_boost.load(glob(join(config['output'], \"Brazil\", \"catboost*\"))[0])\n",
    "svm.load(glob(join(config['output'], \"Brazil\", \"svm*\"))[0])\n",
    "lstm.load(glob(join(config['output'], \"Brazil\", \"lstm*\"))[0])\n",
    "\n",
    "# Finetuned Ensemble in Peru\n",
    "ens.load(glob(join(config['output'], \"Peru\", \"rf*\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fpt-M0gl7SbK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12861,
     "status": "ok",
     "timestamp": 1648569763733,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "fpt-M0gl7SbK",
    "outputId": "624991f7-d77f-4711-d734-31a368951917"
   },
   "outputs": [],
   "source": [
    "trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "trainL, valL = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "catBoost_train = cat_boost.model.predict(trainC[0])\n",
    "catBoost_train[catBoost_train < 0] = 0\n",
    "catBoost_val = cat_boost.model.predict(valC[0])\n",
    "catBoost_val[catBoost_val < 0] = 0\n",
    "\n",
    "svm_train = svm.model.predict(trainC[0])\n",
    "svm_train[svm_train < 0] = 0\n",
    "svm_val = svm.model.predict(valC[0])\n",
    "svm_val[svm_val < 0] = 0\n",
    "\n",
    "lstm_train = lstm.model.predict(trainL[0])\n",
    "lstm_train[lstm_train < 0] = 0\n",
    "lstm_val = lstm.model.predict(valL[0])\n",
    "lstm_val[lstm_val < 0] = 0\n",
    "\n",
    "train_pol0, train_pol1, train_pol2, val_pol0, val_pol1, val_pol2 = interpolation(trainL, valL)\n",
    "\n",
    "x_trainE = np.concatenate((catBoost_train, svm_train, lstm_train, train_pol0, train_pol1, train_pol2), axis = -1)\n",
    "y_trainE = trainingL[1]\n",
    "x_valE = np.concatenate((catBoost_val, svm_val, lstm_val, val_pol0, val_pol1, val_pol2), axis = -1)\n",
    "y_valE = validationL[1]\n",
    "\n",
    "print(x_trainE.shape, y_trainE.shape, x_valE.shape, y_valE.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5F4bagE1W-ys",
   "metadata": {
    "executionInfo": {
     "elapsed": 7318,
     "status": "ok",
     "timestamp": 1648569786360,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "5F4bagE1W-ys"
   },
   "outputs": [],
   "source": [
    "def pred_ints(model, X, percentile=95):\n",
    "    err_down = []\n",
    "    err_up = []\n",
    "    for x in range(len(X)):\n",
    "        preds = []\n",
    "        for pred in model.estimators_:\n",
    "            preds.append(pred.predict(X[x].reshape(1, -1))[0])\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        err_down.append(np.percentile(preds, (100 - percentile) / 2., axis=0))\n",
    "        err_up.append(np.percentile(preds, 100 - (100 - percentile) / 2., axis=0))\n",
    "    return np.array(err_down), np.array(err_up)\n",
    "\n",
    "train_err_down, train_err_up = pred_ints(ens.model, x_trainE, percentile=95)\n",
    "val_err_down, val_err_up = pred_ints(ens.model, x_valE, percentile=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IaSxa8z-4QYt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4951,
     "status": "ok",
     "timestamp": 1648458367112,
     "user": {
      "displayName": "Alessandro Sebastianelli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgVWBLvEG9yF5gq_rkIyj5MCLQv1OzgwlLWxu7e=s64",
      "userId": "16665723407518839496"
     },
     "user_tz": -120
    },
    "id": "IaSxa8z-4QYt",
    "outputId": "26832abd-c2d1-4d34-aa66-6d51891661cd"
   },
   "outputs": [],
   "source": [
    "preds_tra = ens.model.predict(x_trainE)\n",
    "preds_tra[preds_tra < 0] = 0\n",
    "preds_val = ens.model.predict(x_valE)\n",
    "preds_val[preds_val < 0] = 0\n",
    "\n",
    "datelist = pd.date_range('01-01-2010', end='01-01-2020', freq='YS')\n",
    "dates = datelist.strftime(\"%Y\").tolist()\n",
    "# Total timesteps (nb of months inside this time window) = 228\n",
    "month_datelist = pd.date_range('01-01-2010', end='31-12-2019', freq='MS')\n",
    "ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "for i in range(nb_deps):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18,9))\n",
    "\n",
    "    idxt = preds_tra.shape[0]//nb_deps\n",
    "    idxv = preds_val.shape[0]//nb_deps\n",
    "\n",
    "    t1 = ts[12:12+idxt]\n",
    "    t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "    # Reverting Data Norm ---------\n",
    "    pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "    pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    gtt = scaler.inverse_transform(trainL[1][idxt*i:idxt*(i+1), :])\n",
    "    gtv = scaler.inverse_transform(valL[1][idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    lstm_pt = scaler.inverse_transform(lstm_train[idxt*i:idxt*(i+1), :])\n",
    "    lstm_pv = scaler.inverse_transform(lstm_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    svm_pt = scaler.inverse_transform(svm_train[idxt*i:idxt*(i+1), :])\n",
    "    svm_pv = scaler.inverse_transform(svm_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    cb_pt = scaler.inverse_transform(catBoost_train[idxt*i:idxt*(i+1), :])\n",
    "    cb_pv = scaler.inverse_transform(catBoost_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    t_lci = scaler.inverse_transform(train_err_down[idxt*i:idxt*(i+1), :])\n",
    "    t_uci = scaler.inverse_transform(train_err_up[idxt*i:idxt*(i+1), :])\n",
    "\n",
    "    v_lci = scaler.inverse_transform(val_err_down[idxv*i:idxv*(i+1), :])\n",
    "    v_uci = scaler.inverse_transform(val_err_up[idxv*i:idxv*(i+1), :])\n",
    "    # ------------------------------\n",
    "    \n",
    "    baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "    base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "    ########################\n",
    "    ### Total population ###\n",
    "    ########################\n",
    "    axes[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "    axes[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "    axes[0].plot(t1, pt[:,0], '--', color='red', label = 'Ensemble prediction on Training data')\n",
    "    axes[0].fill_between(t1, y1=(pt[:,0]-t_lci[:,0]), y2=(pt[:,0]+t_uci[:,0]), facecolor='red', color='red', linewidth=0.5, alpha=.1, label='Ensemble CI')\n",
    "    axes[0].plot(t2, pv[:,0], '-', color='red', label = 'Ensemble prediction on Validation data')\n",
    "    axes[0].fill_between(t2, y1=(pv[:,0]-v_lci[:,0]), y2=(pv[:,0]+v_uci[:,0]), facecolor='red', color='red', linewidth=0.5, alpha=.1)\n",
    "\n",
    "    # Baseline model\n",
    "    axes[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "    axes[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', color='forestgreen', linewidth=0.5, alpha=.1, label='Baseline model CI')\n",
    "    axes[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', color='forestgreen', linewidth=0.5, alpha=.1)\n",
    "\n",
    "    #####################\n",
    "    ### Children 0-19 ###\n",
    "    #####################\n",
    "    axes[1].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "    axes[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "    axes[1].plot(t1, pt[:,1], '--', color='red', label = 'Ensemble prediction on Training data')\n",
    "    axes[1].fill_between(t1, y1=(pt[:,1]-t_lci[:,1]), y2=(pt[:,1]+t_uci[:,1]), facecolor='red', color='red', linewidth=0.5, alpha=.1, label='Ensemble CI')\n",
    "    axes[1].plot(t2, pv[:,1], '-', color='red', label = 'Ensemble prediction on Validation data')\n",
    "    axes[1].fill_between(t2, y1=(pv[:,1]-v_lci[:,1]), y2=(pv[:,1]+v_uci[:,1]), facecolor='red', color='red', linewidth=0.5, alpha=.1)\n",
    "\n",
    "    # Baseline model\n",
    "    axes[1].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "    axes[1].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', color='forestgreen', linewidth=0.5, alpha=.1, label='Baseline model CI')\n",
    "    axes[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', color='forestgreen', linewidth=0.5, alpha=.1)\n",
    "\n",
    "    # Configure visualization\n",
    "    axes[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left', fontweight=\"bold\")\n",
    "    axes[0].set(ylabel='DIR Total Population')\n",
    "    axes[1].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "    # if i in prob_deps:\n",
    "    #     axes[1].legend(ncol=5, loc='lower center', bbox_to_anchor=(0.5,-0.5), shadow=True, fontsize=12.5)\n",
    "    # else:\n",
    "    axes[1].legend(ncol=3, loc='lower center', bbox_to_anchor=(0.5,-0.5), shadow=True)\n",
    "    axes[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "    axes[1].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "    for ax in axes:\n",
    "        #ax.set_xticks(np.arange(0, idxt+idxv, 11))\n",
    "        ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "        ax.grid(True, linewidth=0.5)\n",
    "        ax.set_xlim(ts[0],ts[-1]+5)\n",
    "    axes[0].set_xticklabels([])\n",
    "    axes[1].set_xticklabels(dates)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print('\\n##########################################################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WA7JBRiVyLxC",
   "metadata": {
    "id": "WA7JBRiVyLxC"
   },
   "source": [
    "**Get time series plots with validation period zoomed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cKbtZp7hTY-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 11043,
     "status": "ok",
     "timestamp": 1648570360450,
     "user": {
      "displayName": "Raquel Carmo",
      "userId": "09214858898201854328"
     },
     "user_tz": -120
    },
    "id": "4cKbtZp7hTY-",
    "outputId": "f266fc13-6829-476d-d824-4c4019e91827"
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "y_rescaled = False\n",
    "\n",
    "preds_tra = ens.model.predict(x_trainE)\n",
    "preds_tra[preds_tra < 0] = 0\n",
    "preds_val = ens.model.predict(x_valE)\n",
    "preds_val[preds_val < 0] = 0\n",
    "\n",
    "datelist = pd.date_range('01-01-2010', end='01-01-2020', freq='YS')\n",
    "dates = datelist.strftime(\"%Y\").tolist()\n",
    "\n",
    "# Total timesteps (nb of months inside this time window) = 228\n",
    "month_datelist = pd.date_range('01-01-2010', end='31-12-2019', freq='MS')\n",
    "ts = np.arange(0, len(month_datelist), 1)\n",
    "val_dates = pd.date_range('01-01-2017', end='31-12-2019', freq = 'M').strftime(\"%m-%Y\").tolist()\n",
    "\n",
    "\n",
    "for i in range(nb_deps):\n",
    "    fig, (axes1, axes2) = plt.subplots(nrows=2, ncols=2, figsize=(22,9), gridspec_kw={'width_ratios': [6, 3], 'wspace':0.01, 'hspace':0.06})\n",
    "\n",
    "    idxt = preds_tra.shape[0]//nb_deps\n",
    "    idxv = preds_val.shape[0]//nb_deps        \n",
    "\n",
    "    t1 = ts[12:12+idxt]\n",
    "    t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "    # Reverting Data Norm ---------\n",
    "    pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "    pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    gtt = scaler.inverse_transform(trainL[1][idxt*i:idxt*(i+1), :])\n",
    "    gtv = scaler.inverse_transform(valL[1][idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    t_lci = scaler.inverse_transform(train_err_down[idxt*i:idxt*(i+1), :])\n",
    "    t_uci = scaler.inverse_transform(train_err_up[idxt*i:idxt*(i+1), :])\n",
    "\n",
    "    v_lci = scaler.inverse_transform(val_err_down[idxv*i:idxv*(i+1), :])\n",
    "    v_uci = scaler.inverse_transform(val_err_up[idxv*i:idxv*(i+1), :])\n",
    "    # ------------------------------\n",
    "    \n",
    "    baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "    base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "    #________________________________________________________________________\n",
    "    # TOTAL POPULATION\n",
    "    axes1[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "    axes1[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "    axes1[0].plot(t1, pt[:,0], '-', color='red', label = 'Ensemble prediction')\n",
    "    axes1[0].fill_between(t1, y1=(pt[:,0]-t_lci[:,0]), y2=(pt[:,0]+t_uci[:,0]), facecolor='red', linewidth=1, alpha=.1, label='Ensemble CI')\n",
    "    axes1[0].plot(t2, pv[:,0], '-', color='red', label = 'Ensemble prediction')\n",
    "    axes1[0].fill_between(t2, y1=(pv[:,0]-v_lci[:,0]), y2=(pv[:,0]+v_uci[:,0]), facecolor='red', linewidth=1, alpha=.1)\n",
    "\n",
    "    # Baseline model\n",
    "    axes1[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "    axes1[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model CI')\n",
    "    axes1[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes1[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "    # Validation Zoom\n",
    "    axes1[1].plot(t2, gtv[:,0], '-', color='orange')\n",
    "    axes1[1].plot(t2, pv[:,0], '-', color='red')\n",
    "    axes1[1].fill_between(t2, y1=(pv[:,0]-v_lci[:,0]), y2=(pv[:,0]+v_uci[:,0]), facecolor='red', alpha=.1)\n",
    "    axes1[1].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes1[1].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "    #________________________________________________________________________\n",
    "    # AGE 0-19\n",
    "    axes2[0].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "    axes2[0].plot(t2, gtv[:,1], '-', color='orange')\n",
    "    axes2[0].plot(t1, pt[:,1], '-', color='red', label = 'Ensemble prediction')\n",
    "    axes2[0].fill_between(t1, y1=(pt[:,1]-t_lci[:,1]), y2=(pt[:,1]+t_uci[:,1]), facecolor='red', alpha=.1, label='Ensemble CI')\n",
    "    axes2[0].plot(t2, pv[:,1], '-', color='red', label = 'Ensemble prediction')\n",
    "    axes2[0].fill_between(t2, y1=(pv[:,1]-v_lci[:,1]), y2=(pv[:,1]+v_uci[:,1]), facecolor='red', alpha=.1)\n",
    "\n",
    "    # Baseline model\n",
    "    axes2[0].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "    axes2[0].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model CI')\n",
    "    axes2[0].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes2[0].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "    # Validation Zoom\n",
    "    axes2[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "    axes2[1].plot(t2, pv[:,1], '-', color='red')\n",
    "    axes2[1].fill_between(t2, y1=(pv[:,1]-v_lci[:,1]), y2=(pv[:,1]+v_uci[:,1]), facecolor='red', alpha=.1)\n",
    "    axes2[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes2[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "    # Configure visualization\n",
    "    axes1[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left', fontweight=\"bold\")\n",
    "    axes1[1].set_title('Validation period', fontsize = 15)\n",
    "    axes1[0].set(ylabel='DIR Total Population')\n",
    "    axes2[0].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "    axes2[1].set(xlabel='Month-Year')\n",
    "    axes1[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "    axes2[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color='orange', lw=2),\n",
    "                       (Line2D([0], [0], color='red', lw=2), Patch(facecolor='red', alpha=.1, lw=2)),\n",
    "                       (Line2D([0], [0], color='forestgreen', lw=2), Patch(facecolor='forestgreen', alpha=.1, lw=2))                             \n",
    "                    ]\n",
    "    axes2[0].legend(handles=legend_elements, labels=['Observed Cases', 'Ensemble Prediction + CI', 'Baseline Prediction + CI'], \n",
    "                    ncol=3, loc='lower center', bbox_to_anchor=(0.75,-0.55), shadow=True)\n",
    "\n",
    "\n",
    "    for ax in [axes1[0], axes2[0]]:\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "        ax.grid(True, linewidth=0.5)\n",
    "        ax.set_xlim(ts[0],ts[-1]+5)\n",
    "\n",
    "    for ax in [axes1[1], axes2[1]]:\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_xticks(np.arange(t2[0],t2[0]+len(t2),3))\n",
    "        ax.yaxis.tick_right()\n",
    "        ax.grid(True, linewidth=0.5)\n",
    "        ax.set_xlim(t2[0]-1,)\n",
    "\n",
    "    if y_rescaled:\n",
    "        ymax = max(np.max(gtt[:,0]), np.max(gtv[:,0]), np.max(pt[:,0]), np.max(pv[:,0]))\n",
    "        ymax_019 = max(np.max(gtt[:,1]), np.max(gtv[:,1]), np.max(pt[:,1]), np.max(pv[:,1]))\n",
    "        ymin = min(np.min(gtt[:,0]), np.min(gtv[:,0]), np.min(pt[:,0]), np.min(pv[:,0]))\n",
    "        ymin_019 = min(np.min(gtt[:,1]), np.min(gtv[:,1]), np.min(pt[:,1]), np.min(pv[:,1]))\n",
    "\n",
    "        val_ymax = max(np.max(gtv[:,0]), np.max(pv[:,0]))\n",
    "        val_ymax_019 = max(np.max(gtv[:,1]), np.max(pv[:,1]))\n",
    "        val_ymin = min(np.min(gtv[:,0]), np.min(pv[:,0]))\n",
    "        val_ymin_019 = min(np.min(gtv[:,1]), np.min(pv[:,1]))\n",
    "\n",
    "        # 5% bottom margin and 10% top margin\n",
    "        minY = ymin-0.05*(ymax-ymin)\n",
    "        maxY = ymax+0.1*(ymax-ymin)\n",
    "        minY_019 = ymin_019-0.05*(ymax_019-ymin_019)\n",
    "        maxY_019 = ymax_019+0.1*(ymax_019-ymin_019)\n",
    "        val_minY = val_ymin-0.05*(val_ymax-val_ymin)\n",
    "        val_maxY = val_ymax+0.1*(val_ymax-val_ymin)\n",
    "        val_minY_019 = val_ymin_019-0.05*(val_ymax_019-val_ymin_019)\n",
    "        val_maxY_019 = val_ymax_019+0.1*(val_ymax_019-val_ymin_019)\n",
    "\n",
    "        axes1[0].set_ylim((minY, maxY))\n",
    "        axes2[0].set_ylim((minY_019, maxY_019))\n",
    "        axes1[1].set_ylim((val_minY, val_maxY))\n",
    "        axes2[1].set_ylim((val_minY_019, val_maxY_019))\n",
    "\n",
    "    axes1[0].set_xticklabels([])\n",
    "    axes2[0].set_xticklabels(dates)\n",
    "    axes1[1].set_xticklabels([])\n",
    "    axes2[1].set_xticklabels(val_dates[::3], rotation=45)  \n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "717e2eae-97ff-475f-bfd5-78e8dcee981d",
    "ea3ca977-edc7-4d92-9ddb-47c7bd6fbaca",
    "52ae33ce-0228-4303-a796-ce8a7c50869a",
    "462125c4-fbb2-4fe7-8cfd-3e023fd5356c",
    "c80-UoybcjRp",
    "6a956d2f-f784-4d7e-953d-dfac8c36b3ba",
    "E3iGL37YB9FU",
    "nhxwEojupvjg",
    "9o0QM6JIGJaC"
   ],
   "name": "AI-Peru.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
